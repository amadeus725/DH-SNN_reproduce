#!/usr/bin/env python3
"""
DH-SNN NeuroVPRï¼ˆç¥ç»è§†è§‰ä½ç½®è¯†åˆ«ï¼‰å®éªŒ
=========================================

åŸºäºSpikingJellyæ¡†æ¶çš„DH-SNN vs æ™®é€šSNNå¯¹æ¯”å®éªŒ
ä½¿ç”¨NeuroVPRæ•°æ®é›†è¿›è¡Œè§†è§‰ä½ç½®è¯†åˆ«ä»»åŠ¡

"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import time
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent.parent.parent
sys.path.append(str(current_dir))

# SpikingJellyå¯¼å…¥
from spikingjelly.activation_based import neuron, functional, layer, surrogate

from dh_snn.utils import setup_seed

print("ğŸš€ DH-SNN NeuroVPRç¥ç»è§†è§‰ä½ç½®è¯†åˆ«å®éªŒ")
print("="*60)

# å®éªŒå‚æ•°
BATCH_SIZE = 16
N_CLASS = 100
LEARNING_RATE = 1e-3
NUM_EPOCHS = 25
NUM_ITER = 40
NUM_BRANCHES = 4
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# DVSåºåˆ—å‚æ•°
SEQ_LEN_DVS = 4
DVS_EXPAND = 3

# ==================== æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ ====================

def create_mock_neurovpr_data():
    """
    åˆ›å»ºæ¨¡æ‹ŸNeuroVPRæ•°æ®ç”¨äºæµ‹è¯•
    æ¨¡æ‹ŸDVSäº‹ä»¶ç›¸æœºæ•°æ®çš„æ—¶ç©ºç‰¹æ€§
    
    è¿”å›:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    print("ğŸ² åˆ›å»ºæ¨¡æ‹ŸNeuroVPRæ•°æ®...")
    
    # æ•°æ®å‚æ•°
    num_train = 1000
    num_test = 200
    input_height = 32
    input_width = 43
    channels = 2  # DVSåŒé€šé“ï¼ˆON/OFFäº‹ä»¶ï¼‰
    sequence_length = SEQ_LEN_DVS * DVS_EXPAND  # æ—¶é—´åºåˆ—é•¿åº¦
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    train_data = torch.zeros(num_train, sequence_length, channels, input_height, input_width)
    train_labels = torch.randint(0, N_CLASS, (num_train,))
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = torch.zeros(num_test, sequence_length, channels, input_height, input_width)
    test_labels = torch.randint(0, N_CLASS, (num_test,))
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ ä½ç½®ç›¸å…³çš„äº‹ä»¶æ¨¡å¼
    for i in range(num_train):
        label = train_labels[i].item()
        
        # ä¸åŒä½ç½®æœ‰ä¸åŒçš„ç©ºé—´æ¨¡å¼
        center_h = (label % 10) * 3 + 5  # ä½ç½®ç›¸å…³çš„å‚ç›´ä¸­å¿ƒ
        center_w = (label // 10) * 4 + 5  # ä½ç½®ç›¸å…³çš„æ°´å¹³ä¸­å¿ƒ
        
        # ä¸ºæ¯ä¸ªæ—¶é—´æ­¥ç”Ÿæˆäº‹ä»¶
        for t in range(sequence_length):
            # åœ¨æ—¶é—´ç»´åº¦ä¸Šæ·»åŠ å˜åŒ–
            time_offset = t * 0.2
            
            # ç”ŸæˆONäº‹ä»¶ï¼ˆé€šé“0ï¼‰
            for _ in range(50 + label % 20):  # ä¸åŒä½ç½®æœ‰ä¸åŒçš„äº‹ä»¶å¯†åº¦
                h = int(np.clip(np.random.normal(center_h + time_offset, 3), 0, input_height-1))
                w = int(np.clip(np.random.normal(center_w, 3), 0, input_width-1))
                train_data[i, t, 0, h, w] = 1.0
            
            # ç”ŸæˆOFFäº‹ä»¶ï¼ˆé€šé“1ï¼‰
            for _ in range(30 + label % 15):
                h = int(np.clip(np.random.normal(center_h - time_offset, 2), 0, input_height-1))
                w = int(np.clip(np.random.normal(center_w + 1, 2), 0, input_width-1))
                train_data[i, t, 1, h, w] = 1.0
    
    # ä¸ºæµ‹è¯•æ•°æ®ç”Ÿæˆç±»ä¼¼æ¨¡å¼
    for i in range(num_test):
        label = test_labels[i].item()
        center_h = (label % 10) * 3 + 5
        center_w = (label // 10) * 4 + 5
        
        for t in range(sequence_length):
            time_offset = t * 0.2
            
            # ONäº‹ä»¶
            for _ in range(50 + label % 20):
                h = int(np.clip(np.random.normal(center_h + time_offset, 3), 0, input_height-1))
                w = int(np.clip(np.random.normal(center_w, 3), 0, input_width-1))
                test_data[i, t, 0, h, w] = 1.0
            
            # OFFäº‹ä»¶
            for _ in range(30 + label % 15):
                h = int(np.clip(np.random.normal(center_h - time_offset, 2), 0, input_height-1))
                w = int(np.clip(np.random.normal(center_w + 1, 2), 0, input_width-1))
                test_data[i, t, 1, h, w] = 1.0
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)
    test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=BATCH_SIZE, shuffle=False
    )
    
    print(f"   æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®: {train_data.shape}")
    print(f"   æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®: {test_data.shape}")
    print(f"   è¾“å…¥ç‰¹å¾ç»´åº¦: {sequence_length * channels * input_height * input_width}")
    
    return train_loader, test_loader

# ==================== ç²¾åº¦è®¡ç®—å‡½æ•° ====================

def accuracy(output, target, topk=(1,)):
    """
    è®¡ç®—Top-Kå‡†ç¡®ç‡
    
    å‚æ•°:
        output: æ¨¡å‹è¾“å‡ºï¼Œå½¢çŠ¶ä¸º[batch_size, num_classes]
        target: çœŸå®æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º[batch_size]
        topk: Kå€¼å…ƒç»„
        
    è¿”å›:
        res: å„ä¸ªKå€¼å¯¹åº”çš„å‡†ç¡®ç‡
    """
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size).item())
        return res

# ==================== æ¨¡å‹å®šä¹‰ ====================

class NeuroVPR_DH_SNN(nn.Module):
    """
    ç”¨äºNeuroVPRä»»åŠ¡çš„DH-SNNæ¨¡å‹
    å¤„ç†DVSäº‹ä»¶ç›¸æœºçš„æ—¶ç©ºæ•°æ®
    """
    
    def __init__(self, input_dim, hidden_dims=[256, 256], output_dim=N_CLASS, num_branches=NUM_BRANCHES):
        """
        åˆå§‹åŒ–NeuroVPR DH-SNNæ¨¡å‹
        
        å‚æ•°:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            output_dim: è¾“å‡ºç±»åˆ«æ•°
            num_branches: æ ‘çªåˆ†æ”¯æ•°é‡
        """
        super(NeuroVPR_DH_SNN, self).__init__()
        
        print(f"ğŸ—ï¸  åˆ›å»ºNeuroVPR DH-SNNæ¨¡å‹:")
        print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"   éšè—å±‚: {hidden_dims}")
        print(f"   è¾“å‡ºç»´åº¦: {output_dim}")
        print(f"   åˆ†æ”¯æ•°é‡: {num_branches}")
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.num_branches = num_branches
        
        # æ„å»ºå¤šå±‚DH-SNNç½‘ç»œ
        self.layers = nn.ModuleList()
        
        current_dim = input_dim
        for i, hidden_dim in enumerate(hidden_dims):
            # åˆ†æ”¯çº¿æ€§å±‚
            branch_layers = nn.ModuleList()
            for j in range(num_branches):
                branch_layers.append(layer.Linear(current_dim // num_branches, hidden_dim // num_branches, bias=False))
            self.layers.append(branch_layers)
            
            # æ ‘çªæ—¶é—´å¸¸æ•°å‚æ•°
            tau_n = nn.Parameter(torch.empty(num_branches, hidden_dim).uniform_(0, 4))
            self.register_parameter(f'tau_n_{i}', tau_n)
            
            # è†œç”µä½æ—¶é—´å¸¸æ•°å‚æ•°
            tau_m = nn.Parameter(torch.empty(hidden_dim).uniform_(0, 4))
            self.register_parameter(f'tau_m_{i}', tau_m)
            
            current_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        self.output_layer = layer.Linear(current_dim, output_dim)
        
        # ç¥ç»å…ƒçŠ¶æ€ç¼“å­˜
        self.reset_states()
        
        print("âœ… NeuroVPR DH-SNNæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    def reset_states(self):
        """é‡ç½®æ‰€æœ‰ç¥ç»å…ƒçŠ¶æ€"""
        self.dendritic_currents = []
        self.membrane_potentials = []
        self.spike_outputs = []
        
        for i, hidden_dim in enumerate(self.hidden_dims):
            self.dendritic_currents.append([None] * self.num_branches)
            self.membrane_potentials.append(None)
            self.spike_outputs.append(None)
    
    def set_batch_size(self, batch_size):
        """è®¾ç½®æ‰¹æ¬¡å¤§å°å¹¶åˆå§‹åŒ–çŠ¶æ€"""
        for i, hidden_dim in enumerate(self.hidden_dims):
            # åˆå§‹åŒ–æ ‘çªç”µæµ
            for j in range(self.num_branches):
                self.dendritic_currents[i][j] = torch.zeros(batch_size, hidden_dim // self.num_branches).to(DEVICE)
            
            # åˆå§‹åŒ–è†œç”µä½å’Œè„‰å†²è¾“å‡º
            self.membrane_potentials[i] = torch.rand(batch_size, hidden_dim).to(DEVICE)
            self.spike_outputs[i] = torch.zeros(batch_size, hidden_dim).to(DEVICE)
    
    def surrogate_gradient(self, x):
        """ä»£ç†æ¢¯åº¦å‡½æ•°"""
        return (x > 0).float() + 0.5 * torch.tanh(2 * x) * (1 - (x > 0).float())
    
    def forward_layer(self, x, layer_idx):
        """å•å±‚å‰å‘ä¼ æ’­"""
        batch_size = x.size(0)
        
        # åˆ†å‰²è¾“å…¥åˆ°å„ä¸ªåˆ†æ”¯
        input_splits = torch.chunk(x, self.num_branches, dim=1)
        
        # å¤„ç†å„ä¸ªåˆ†æ”¯
        branch_outputs = []
        for j in range(self.num_branches):
            # åˆ†æ”¯çº¿æ€§å˜æ¢
            branch_input = self.layers[layer_idx][j](input_splits[j])
            
            # æ ‘çªæ—¶é—´å¸¸æ•°
            tau_n = getattr(self, f'tau_n_{layer_idx}')
            beta = torch.sigmoid(tau_n[j])
            
            # æ›´æ–°æ ‘çªç”µæµ
            self.dendritic_currents[layer_idx][j] = (
                beta * self.dendritic_currents[layer_idx][j] + 
                (1 - beta) * branch_input
            )
            
            branch_outputs.append(self.dendritic_currents[layer_idx][j])
        
        # æ±‡æ€»åˆ†æ”¯è¾“å‡º
        total_current = torch.cat(branch_outputs, dim=1)
        
        # è†œç”µä½æ›´æ–°
        tau_m = getattr(self, f'tau_m_{layer_idx}')
        alpha = torch.sigmoid(tau_m)
        
        self.membrane_potentials[layer_idx] = (
            alpha * self.membrane_potentials[layer_idx] + 
            (1 - alpha) * total_current - 
            self.spike_outputs[layer_idx]
        )
        
        # è„‰å†²ç”Ÿæˆ
        spike_input = self.membrane_potentials[layer_idx] - 1.0
        self.spike_outputs[layer_idx] = self.surrogate_gradient(spike_input)
        
        return self.spike_outputs[layer_idx]
    
    def forward(self, dvs_input):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            dvs_input: DVSè¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ä¸º[batch, seq_len, channels, height, width]
            
        è¿”å›:
            output: åˆ†ç±»è¾“å‡º
        """
        batch_size, seq_len, channels, height, width = dvs_input.shape
        
        # è®¾ç½®æ‰¹æ¬¡å¤§å°
        self.set_batch_size(batch_size)
        
        # å°†DVSæ•°æ®é‡å¡‘ä¸ºç‰¹å¾å‘é‡
        dvs_reshaped = dvs_input.view(batch_size, seq_len, -1)  # [batch, seq_len, features]
        
        outputs = []
        for t in range(seq_len):
            x = dvs_reshaped[:, t, :]  # [batch, features]
            
            # é€å±‚å‰å‘ä¼ æ’­
            for layer_idx in range(len(self.hidden_dims)):
                x = self.forward_layer(x, layer_idx)
            
            # è¾“å‡ºå±‚
            output = self.output_layer(x)
            outputs.append(output)
        
        # æ—¶é—´ç»´åº¦èšåˆ - ä½¿ç”¨æœ€åå‡ ä¸ªæ—¶é—´æ­¥çš„å¹³å‡
        final_output = torch.stack(outputs[-3:], dim=1).mean(dim=1)
        
        return final_output

class NeuroVPR_Vanilla_SNN(nn.Module):
    """
    ç”¨äºNeuroVPRä»»åŠ¡çš„æ™®é€šSNNæ¨¡å‹
    """
    
    def __init__(self, input_dim, hidden_dims=[256, 256], output_dim=N_CLASS):
        """
        åˆå§‹åŒ–NeuroVPR æ™®é€šSNNæ¨¡å‹
        
        å‚æ•°:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            output_dim: è¾“å‡ºç±»åˆ«æ•°
        """
        super(NeuroVPR_Vanilla_SNN, self).__init__()
        
        print(f"ğŸ—ï¸  åˆ›å»ºNeuroVPR æ™®é€šSNNæ¨¡å‹:")
        print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"   éšè—å±‚: {hidden_dims}")
        print(f"   è¾“å‡ºç»´åº¦: {output_dim}")
        
        # æ„å»ºæ™®é€šSNNç½‘ç»œ
        self.layers = nn.ModuleList()
        
        current_dim = input_dim
        for hidden_dim in hidden_dims:
            # çº¿æ€§å±‚
            self.layers.append(layer.Linear(current_dim, hidden_dim))
            # LIFç¥ç»å…ƒ
            self.layers.append(neuron.LIFNode(
                tau=2.0,
                v_threshold=1.0,
                surrogate_function=surrogate.ATan(),
                step_mode='s'
            ))
            current_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        self.layers.append(layer.Linear(current_dim, output_dim))
        self.layers.append(neuron.LIFNode(
            tau=2.0,
            v_threshold=1.0,
            surrogate_function=surrogate.ATan(),
            step_mode='s'
        ))
        
        print("âœ… NeuroVPR æ™®é€šSNNæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    def forward(self, dvs_input):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            dvs_input: DVSè¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ä¸º[batch, seq_len, channels, height, width]
            
        è¿”å›:
            output: åˆ†ç±»è¾“å‡º
        """
        batch_size, seq_len, channels, height, width = dvs_input.shape
        
        # å°†DVSæ•°æ®é‡å¡‘ä¸ºç‰¹å¾å‘é‡
        dvs_reshaped = dvs_input.view(batch_size, seq_len, -1)  # [batch, seq_len, features]
        
        outputs = []
        for t in range(seq_len):
            x = dvs_reshaped[:, t, :]  # [batch, features]
            
            # é‡ç½®ç¥ç»å…ƒçŠ¶æ€
            functional.reset_net(self)
            
            # é€å±‚å‰å‘ä¼ æ’­
            for layer in self.layers:
                x = layer(x)
            
            outputs.append(x)
        
        # æ—¶é—´ç»´åº¦èšåˆ
        final_output = torch.stack(outputs[-3:], dim=1).mean(dim=1)
        
        return final_output

# ==================== è®­ç»ƒå‡½æ•° ====================

def train_neurovpr_model(model, train_loader, test_loader, model_name, num_epochs=NUM_EPOCHS):
    """
    è®­ç»ƒNeuroVPRæ¨¡å‹
    
    å‚æ•°:
        model: å¾…è®­ç»ƒçš„æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        model_name: æ¨¡å‹åç§°
        num_epochs: è®­ç»ƒè½®æ•°
        
    è¿”å›:
        results: è®­ç»ƒç»“æœå­—å…¸
    """
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {model_name}")
    print("-" * 50)
    
    model = model.to(DEVICE)
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    criterion = nn.CrossEntropyLoss()
    
    # è®­ç»ƒæŒ‡æ ‡
    train_losses = []
    test_accuracies = []
    best_test_acc1 = 0.0
    best_test_acc5 = 0.0
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        running_loss = 0.0
        train_acc1 = 0.0
        train_acc5 = 0.0
        train_batches = 0
        
        for batch_idx, (dvs_data, target) in enumerate(train_loader):
            if batch_idx >= NUM_ITER:  # é™åˆ¶æ¯è½®è¿­ä»£æ¬¡æ•°
                break
                
            dvs_data, target = dvs_data.to(DEVICE), target.to(DEVICE)
            
            # é‡ç½®æ¨¡å‹çŠ¶æ€
            if hasattr(model, 'reset_states'):
                model.reset_states()
            
            optimizer.zero_grad()
            outputs = model(dvs_data)
            loss = criterion(outputs, target)
            
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            running_loss += loss.item()
            
            # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
            acc1, acc5 = accuracy(outputs.cpu(), target.cpu(), topk=(1, 5))
            train_acc1 += acc1
            train_acc5 += acc5
            train_batches += 1
        
        lr_scheduler.step()
        
        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_loss = 0.0
        test_acc1 = 0.0
        test_acc5 = 0.0
        test_batches = 0
        
        with torch.no_grad():
            for dvs_data, target in test_loader:
                dvs_data, target = dvs_data.to(DEVICE), target.to(DEVICE)
                
                if hasattr(model, 'reset_states'):
                    model.reset_states()
                
                outputs = model(dvs_data)
                loss = criterion(outputs, target)
                test_loss += loss.item()
                
                acc1, acc5 = accuracy(outputs.cpu(), target.cpu(), topk=(1, 5))
                test_acc1 += acc1
                test_acc5 += acc5
                test_batches += 1
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_train_loss = running_loss / train_batches
        avg_train_acc1 = train_acc1 / train_batches
        avg_train_acc5 = train_acc5 / train_batches
        avg_test_loss = test_loss / test_batches
        avg_test_acc1 = test_acc1 / test_batches
        avg_test_acc5 = test_acc5 / test_batches
        
        train_losses.append(avg_train_loss)
        test_accuracies.append(avg_test_acc1)
        
        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
        if avg_test_acc1 > best_test_acc1:
            best_test_acc1 = avg_test_acc1
        if avg_test_acc5 > best_test_acc5:
            best_test_acc5 = avg_test_acc5
        
        # æ‰“å°è¿›åº¦
        if epoch % 5 == 0 or epoch == num_epochs - 1:
            print(f'è½®æ¬¡ [{epoch+1}/{num_epochs}]:')
            print(f'  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒAcc1: {avg_train_acc1:.2f}%')
            print(f'  æµ‹è¯•æŸå¤±: {avg_test_loss:.4f}, æµ‹è¯•Acc1: {avg_test_acc1:.2f}%, æµ‹è¯•Acc5: {avg_test_acc5:.2f}%')
            print(f'  æœ€ä½³æµ‹è¯•Acc1: {best_test_acc1:.2f}%, æœ€ä½³æµ‹è¯•Acc5: {best_test_acc5:.2f}%')
    
    return {
        'train_losses': train_losses,
        'test_accuracies': test_accuracies,
        'best_test_acc1': best_test_acc1,
        'best_test_acc5': best_test_acc5,
        'final_test_acc1': avg_test_acc1,
        'final_test_acc5': avg_test_acc5
    }

# ==================== ä¸»å®éªŒå‡½æ•° ====================

def run_neurovpr_experiment():
    """è¿è¡ŒNeuroVPRå®éªŒ"""
    
    print("=" * 80)
    print("ğŸ‘ï¸  DH-SNN NeuroVPRç¥ç»è§†è§‰ä½ç½®è¯†åˆ«å®éªŒ")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    setup_seed(42)
    
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        train_loader, test_loader = create_mock_neurovpr_data()
        
        # è®¡ç®—è¾“å…¥ç‰¹å¾ç»´åº¦
        sample_data, _ = next(iter(train_loader))
        input_features = sample_data.view(sample_data.shape[0], -1).shape[1] // SEQ_LEN_DVS // DVS_EXPAND
        print(f"   è¾“å…¥ç‰¹å¾ç»´åº¦: {input_features}")
        
        # åˆ›å»ºæ¨¡å‹
        print(f"\nğŸ—ï¸  åœ¨ {DEVICE} ä¸Šåˆå§‹åŒ–æ¨¡å‹...")
        
        # DH-SNNæ¨¡å‹
        dh_snn_model = NeuroVPR_DH_SNN(
            input_dim=input_features,
            hidden_dims=[256, 256],
            output_dim=N_CLASS,
            num_branches=NUM_BRANCHES
        )
        
        # æ™®é€šSNNæ¨¡å‹
        vanilla_snn_model = NeuroVPR_Vanilla_SNN(
            input_dim=input_features,
            hidden_dims=[256, 256],
            output_dim=N_CLASS
        )
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   DH-SNNå‚æ•°: {sum(p.numel() for p in dh_snn_model.parameters()):,}")
        print(f"   æ™®é€šSNNå‚æ•°: {sum(p.numel() for p in vanilla_snn_model.parameters()):,}")
        
        # å¼€å§‹è®­ç»ƒå®éªŒ
        print(f"\nğŸ”¬ å¼€å§‹è®­ç»ƒå®éªŒ...")
        
        # è®­ç»ƒDH-SNN
        dh_results = train_neurovpr_model(dh_snn_model, train_loader, test_loader, "DH-SNN")
        
        # è®­ç»ƒæ™®é€šSNN
        vanilla_results = train_neurovpr_model(vanilla_snn_model, train_loader, test_loader, "æ™®é€šSNN")
        
        # ç»“æœå¯¹æ¯”
        print("\n" + "=" * 80)
        print("ğŸ¯ æœ€ç»ˆç»“æœå¯¹æ¯”")
        print("=" * 80)
        
        print(f"DH-SNNç»“æœ:")
        print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {dh_results['best_test_acc1']:.2f}%")
        print(f"  æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {dh_results['final_test_acc1']:.2f}%")
        print(f"  æœ€ä½³Top-5å‡†ç¡®ç‡: {dh_results['best_test_acc5']:.2f}%")
        
        print(f"\næ™®é€šSNNç»“æœ:")
        print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {vanilla_results['best_test_acc1']:.2f}%")
        print(f"  æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {vanilla_results['final_test_acc1']:.2f}%")
        print(f"  æœ€ä½³Top-5å‡†ç¡®ç‡: {vanilla_results['best_test_acc5']:.2f}%")
        
        # è®¡ç®—æ”¹è¿›
        best_improvement = dh_results['best_test_acc1'] - vanilla_results['best_test_acc1']
        final_improvement = dh_results['final_test_acc1'] - vanilla_results['final_test_acc1']
        
        if vanilla_results['best_test_acc1'] > 0:
            best_relative = (dh_results['best_test_acc1'] / vanilla_results['best_test_acc1'] - 1) * 100
        else:
            best_relative = 0
            
        if vanilla_results['final_test_acc1'] > 0:
            final_relative = (dh_results['final_test_acc1'] / vanilla_results['final_test_acc1'] - 1) * 100
        else:
            final_relative = 0
        
        print(f"\nğŸ“ˆ æ€§èƒ½æ”¹è¿›:")
        print(f"  æœ€ä½³å‡†ç¡®ç‡: +{best_improvement:.2f}% (ç›¸å¯¹: +{best_relative:.1f}%)")
        print(f"  æœ€ç»ˆå‡†ç¡®ç‡: +{final_improvement:.2f}% (ç›¸å¯¹: +{final_relative:.1f}%)")
        
        # ä¿å­˜ç»“æœ
        results_path = Path("results/neurovpr_experiment_results.json")
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        all_results = {
            'experiment_info': {
                'name': 'NeuroVPRç¥ç»è§†è§‰ä½ç½®è¯†åˆ«å®éªŒ',
                'framework': 'SpikingJelly + DH-SNN',
                'date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'dataset': 'NeuroVPR (æ¨¡æ‹Ÿæ•°æ®)',
                'num_classes': N_CLASS,
                'batch_size': BATCH_SIZE,
                'num_epochs': NUM_EPOCHS,
                'num_branches': NUM_BRANCHES,
                'device': str(DEVICE)
            },
            'dh_snn': dh_results,
            'vanilla_snn': vanilla_results,
            'comparison': {
                'best_accuracy_improvement': {
                    'absolute': best_improvement,
                    'relative_percent': best_relative
                },
                'final_accuracy_improvement': {
                    'absolute': final_improvement,
                    'relative_percent': final_relative
                }
            }
        }
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
        print(f"\nğŸ“ˆ ä¸è®ºæ–‡ç»“æœå¯¹æ¯”:")
        print(f"è®ºæ–‡NeuroVPRä»»åŠ¡ä¸­DH-SNNå±•ç°å‡ºäº†ä¼˜è¶Šæ€§")
        
        if best_improvement > 2:
            print("ğŸ‰ DH-SNNæ˜¾è‘—ä¼˜äºæ™®é€šSNN - ç¬¦åˆé¢„æœŸ!")
        elif best_improvement > 0:
            print("âœ… DH-SNNä¼˜äºæ™®é€šSNN")
        else:
            print("âš ï¸  ç»“æœéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
        
        return all_results
        
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = run_neurovpr_experiment()
    if results:
        print(f"\nğŸ NeuroVPRå®éªŒæˆåŠŸå®Œæˆ!")
    else:
        print(f"\nâŒ NeuroVPRå®éªŒå¤±è´¥")
