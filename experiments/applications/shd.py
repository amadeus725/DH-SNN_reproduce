#!/usr/bin/env python3
"""
DH-SNN SHDï¼ˆè„‰å†²æµ·å¾·å ¡æ•°å­—ï¼‰å®éªŒ
====================================

åŸºäºSpikingJellyæ¡†æ¶çš„DH-SNN vs æ™®é€šSNNå¯¹æ¯”å®éªŒ
ä½¿ç”¨SHDæ•°æ®é›†è¿›è¡Œè„‰å†²æ•°å­—è¯†åˆ«ä»»åŠ¡

"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import tables
import time
import json
import math
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent.parent.parent
sys.path.append(str(current_dir))

# SpikingJellyå¯¼å…¥
from spikingjelly.activation_based import neuron, functional, layer, surrogate

from dh_snn.utils import setup_seed

print("ğŸš€ DH-SNN SHDè„‰å†²æ•°å­—è¯†åˆ«å®éªŒ")
print("="*60)

# å®éªŒé…ç½®
CONFIG = {
    'learning_rate': 1e-2,
    'batch_size': 100,
    'epochs': 100,
    'hidden_size': 64,
    'v_threshold': 1.0,
    'dt': 1,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

# æ—¶é—´å› å­é…ç½® - æŒ‰ç…§è®ºæ–‡Table S3
TIMING_CONFIGS = {
    'Small': {'tau_m': (-4.0, 0.0), 'tau_n': (-4.0, 0.0)},   # Î²Ì‚,Î±Ì‚ ~ U(-4,0)
    'Medium': {'tau_m': (0.0, 4.0), 'tau_n': (0.0, 4.0)},    # Î²Ì‚,Î±Ì‚ ~ U(0,4)
    'Large': {'tau_m': (2.0, 6.0), 'tau_n': (2.0, 6.0)}      # Î²Ì‚,Î±Ì‚ ~ U(2,6)
}

# ==================== å¤šé«˜æ–¯æ›¿ä»£å‡½æ•° ====================

class MultiGaussianSurrogate(torch.autograd.Function):
    """
    å¤šé«˜æ–¯æ›¿ä»£å‡½æ•°
    å®Œå…¨æŒ‰ç…§åŸè®ºæ–‡å®ç°çš„MultiGaussianæ›¿ä»£å‡½æ•°
    """

    @staticmethod
    def forward(ctx, input):
        """å‰å‘ä¼ æ’­ï¼šé˜¶è·ƒå‡½æ•°"""
        ctx.save_for_backward(input)
        return input.gt(0).float()

    @staticmethod
    def backward(ctx, grad_output):
        """åå‘ä¼ æ’­ï¼šå¤šé«˜æ–¯è¿‘ä¼¼æ¢¯åº¦"""
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()

        # åŸè®ºæ–‡å‚æ•°: lens=0.5, scale=6.0, height=0.15, gamma=0.5
        lens = 0.5
        scale = 6.0
        height = 0.15
        gamma = 0.5

        def gaussian(x, mu=0., sigma=0.5):
            """é«˜æ–¯å‡½æ•°"""
            return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma

        # MultiGaussian - å®Œå…¨æŒ‰ç…§åŸè®ºæ–‡å…¬å¼
        temp = gaussian(input, mu=0., sigma=lens) * (1. + height) \
             - gaussian(input, mu=lens, sigma=scale * lens) * height \
             - gaussian(input, mu=-lens, sigma=scale * lens) * height

        return grad_input * temp.float() * gamma

multi_gaussian_surrogate = MultiGaussianSurrogate.apply

# ==================== ç¥ç»å…ƒæ¨¡å‹ ====================

class DH_LIFNode(nn.Module):
    """
    æ ‘çªå¼‚è´¨æ€§LIFç¥ç»å…ƒ
    ç­‰ä»·äºåŸè®ºæ–‡çš„LIFç¥ç»å…ƒå®ç°
    """

    def __init__(self, size, tau_m_range=(0.0, 4.0), v_threshold=1.0, device='cpu'):
        """
        åˆå§‹åŒ–DH-LIFç¥ç»å…ƒ
        
        å‚æ•°:
            size: ç¥ç»å…ƒæ•°é‡
            tau_m_range: è†œç”µä½æ—¶é—´å¸¸æ•°åˆå§‹åŒ–èŒƒå›´
            v_threshold: è„‰å†²é˜ˆå€¼
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.size = size
        self.v_threshold = v_threshold
        self.device = device

        # è†œç”µä½æ—¶é—´å¸¸æ•°å‚æ•°
        self.tau_m = nn.Parameter(torch.empty(size))
        nn.init.uniform_(self.tau_m, tau_m_range[0], tau_m_range[1])

        # ç¥ç»å…ƒçŠ¶æ€ç¼“å­˜
        self.register_buffer('mem', torch.zeros(1, size))
        self.register_buffer('spike', torch.zeros(1, size))

    def set_neuron_state(self, batch_size):
        """
        é‡ç½®ç¥ç»å…ƒçŠ¶æ€
        
        å‚æ•°:
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        self.mem = torch.rand(batch_size, self.size).to(self.device)
        self.spike = torch.rand(batch_size, self.size).to(self.device)

    def forward(self, input_current):
        """
        å‰å‘ä¼ æ’­ - å®Œå…¨æŒ‰ç…§åŸè®ºæ–‡mem_update_praå®ç°
        
        å‚æ•°:
            input_current: è¾“å…¥ç”µæµ
            
        è¿”å›:
            mem: è†œç”µä½
            spike: è¾“å‡ºè„‰å†²
        """
        # åŸè®ºæ–‡: alpha = torch.sigmoid(tau_m)
        alpha = torch.sigmoid(self.tau_m)

        # åŸè®ºæ–‡: mem = mem * alpha + (1 - alpha) * R_m * inputs - v_th * spike
        # R_m = 1 åœ¨åŸè®ºæ–‡ä¸­
        self.mem = self.mem * alpha + (1 - alpha) * input_current - self.v_threshold * self.spike

        # åŸè®ºæ–‡: inputs_ = mem - v_th
        inputs_ = self.mem - self.v_threshold

        # åŸè®ºæ–‡: spike = act_fun_adp(inputs_)
        self.spike = multi_gaussian_surrogate(inputs_)

        return self.mem, self.spike

class ReadoutIntegrator(nn.Module):
    """
    è¯»å‡ºç§¯åˆ†å™¨
    ç­‰ä»·äºåŸè®ºæ–‡çš„readout_integrator_test
    """

    def __init__(self, input_dim, output_dim, tau_m_range=(0.0, 4.0), device='cpu'):
        """
        åˆå§‹åŒ–è¯»å‡ºç§¯åˆ†å™¨
        
        å‚æ•°:
            input_dim: è¾“å…¥ç»´åº¦
            output_dim: è¾“å‡ºç»´åº¦  
            tau_m_range: æ—¶é—´å¸¸æ•°åˆå§‹åŒ–èŒƒå›´
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.device = device

        # çº¿æ€§å±‚
        self.dense = nn.Linear(input_dim, output_dim)

        # æ—¶é—´å¸¸æ•°
        self.tau_m = nn.Parameter(torch.empty(output_dim))
        nn.init.uniform_(self.tau_m, tau_m_range[0], tau_m_range[1])

        # è†œç”µä½çŠ¶æ€
        self.register_buffer('mem', torch.zeros(1, output_dim))

    def set_neuron_state(self, batch_size):
        """é‡ç½®ç¥ç»å…ƒçŠ¶æ€"""
        self.mem = torch.rand(batch_size, self.output_dim).to(self.device)

    def forward(self, input_spike):
        """
        å‰å‘ä¼ æ’­ - å®Œå…¨æŒ‰ç…§åŸè®ºæ–‡output_Neuron_praå®ç°
        
        å‚æ•°:
            input_spike: è¾“å…¥è„‰å†²
            
        è¿”å›:
            mem: è†œç”µä½ï¼ˆä¸äº§ç”Ÿè„‰å†²ï¼‰
        """
        # çªè§¦è¾“å…¥
        d_input = self.dense(input_spike.float())

        # åŸè®ºæ–‡: alpha = torch.sigmoid(tau_m)
        alpha = torch.sigmoid(self.tau_m)

        # åŸè®ºæ–‡: mem = mem * alpha + (1-alpha) * inputs
        self.mem = self.mem * alpha + (1 - alpha) * d_input

        return self.mem

class DH_DendriticLayer(nn.Module):
    """
    æ ‘çªå¼‚è´¨æ€§å±‚
    ç­‰ä»·äºåŸè®ºæ–‡çš„spike_dense_test_denri_wotanh_R
    """

    def __init__(self, input_dim, output_dim, tau_m_range=(0.0, 4.0), tau_n_range=(2.0, 6.0),
                 num_branches=4, v_threshold=1.0, device='cpu'):
        """
        åˆå§‹åŒ–æ ‘çªå¼‚è´¨æ€§å±‚
        
        å‚æ•°:
            input_dim: è¾“å…¥ç»´åº¦
            output_dim: è¾“å‡ºç»´åº¦
            tau_m_range: è†œç”µä½æ—¶é—´å¸¸æ•°èŒƒå›´
            tau_n_range: æ ‘çªæ—¶é—´å¸¸æ•°èŒƒå›´
            num_branches: æ ‘çªåˆ†æ”¯æ•°é‡
            v_threshold: è„‰å†²é˜ˆå€¼
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_branches = num_branches
        self.v_threshold = v_threshold
        self.device = device

        # è¿æ¥å±‚ - æŒ‰ç…§åŸè®ºæ–‡å®ç°
        self.pad = ((input_dim) // num_branches * num_branches + num_branches - input_dim) % num_branches
        self.dense = nn.Linear(input_dim + self.pad, output_dim * num_branches)

        # æ—¶é—´å¸¸æ•°å‚æ•°
        self.tau_m = nn.Parameter(torch.empty(output_dim))
        self.tau_n = nn.Parameter(torch.empty(output_dim, num_branches))

        # åˆå§‹åŒ–æ—¶é—´å¸¸æ•°
        nn.init.uniform_(self.tau_m, tau_m_range[0], tau_m_range[1])
        nn.init.uniform_(self.tau_n, tau_n_range[0], tau_n_range[1])

        # ç¥ç»å…ƒçŠ¶æ€
        self.register_buffer('mem', torch.zeros(1, output_dim))
        self.register_buffer('spike', torch.zeros(1, output_dim))
        self.register_buffer('d_input', torch.zeros(1, output_dim, num_branches))

        # åˆ›å»ºè¿æ¥æ©ç 
        self.create_mask()

    def create_mask(self):
        """
        åˆ›å»ºè¿æ¥æ©ç  - å®Œå…¨æŒ‰ç…§åŸè®ºæ–‡å®ç°
        å®ç°ç¨€ç–çš„æ ‘çªè¿æ¥æ¨¡å¼
        """
        input_size = self.input_dim + self.pad
        self.mask = torch.zeros(self.output_dim * self.num_branches, input_size).to(self.device)
        
        for i in range(self.output_dim):
            for j in range(self.num_branches):
                start_idx = j * input_size // self.num_branches
                end_idx = (j + 1) * input_size // self.num_branches
                self.mask[i * self.num_branches + j, start_idx:end_idx] = 1

    def apply_mask(self):
        """åº”ç”¨è¿æ¥æ©ç åˆ°æƒé‡"""
        self.dense.weight.data = self.dense.weight.data * self.mask

    def set_neuron_state(self, batch_size):
        """é‡ç½®ç¥ç»å…ƒçŠ¶æ€"""
        self.mem = torch.rand(batch_size, self.output_dim).to(self.device)
        self.spike = torch.rand(batch_size, self.output_dim).to(self.device)
        self.d_input = torch.zeros(batch_size, self.output_dim, self.num_branches).to(self.device)

    def forward(self, input_spike):
        """
        å‰å‘ä¼ æ’­ - å®Œå…¨æŒ‰ç…§åŸè®ºæ–‡å®ç°
        
        å‚æ•°:
            input_spike: è¾“å…¥è„‰å†²
            
        è¿”å›:
            mem: è†œç”µä½
            spike: è¾“å‡ºè„‰å†²
        """
        # æ ‘çªæ—¶é—´å¸¸æ•°
        beta = torch.sigmoid(self.tau_n)

        # è¾“å…¥å¡«å……
        padding = torch.zeros(input_spike.size(0), self.pad).to(self.device)
        k_input = torch.cat((input_spike.float(), padding), 1)

        # æ›´æ–°æ ‘çªç”µæµ
        dense_output = self.dense(k_input).reshape(-1, self.output_dim, self.num_branches)
        self.d_input = beta * self.d_input + (1 - beta) * dense_output

        # æ€»è¾“å…¥ç”µæµ
        l_input = self.d_input.sum(dim=2, keepdim=False)

        # è†œç”µä½æ›´æ–° - æŒ‰ç…§åŸè®ºæ–‡mem_update_pra
        alpha = torch.sigmoid(self.tau_m)
        self.mem = self.mem * alpha + (1 - alpha) * l_input - self.v_threshold * self.spike

        # è„‰å†²ç”Ÿæˆ
        inputs_ = self.mem - self.v_threshold
        self.spike = multi_gaussian_surrogate(inputs_)

        return self.mem, self.spike

# ==================== ç½‘ç»œæ¨¡å‹ ====================

class VanillaSFNN(nn.Module):
    """
    æ™®é€šè„‰å†²å‰é¦ˆç¥ç»ç½‘ç»œ
    ç­‰ä»·äºåŸè®ºæ–‡çš„spike_dense_test_origin
    """

    def __init__(self, config, tau_m_range=(0.0, 4.0)):
        """
        åˆå§‹åŒ–æ™®é€šSFNN
        
        å‚æ•°:
            config: å®éªŒé…ç½®
            tau_m_range: è†œç”µä½æ—¶é—´å¸¸æ•°èŒƒå›´
        """
        super().__init__()
        self.config = config
        self.device = config['device']

        print(f"ğŸ—ï¸  åˆ›å»ºæ™®é€šSFNNæ¨¡å‹:")
        print(f"   éšè—å±‚å¤§å°: {config['hidden_size']}")
        print(f"   tau_mèŒƒå›´: {tau_m_range}")

        # çº¿æ€§å±‚
        self.dense = nn.Linear(700, config['hidden_size'])

        # LIFç¥ç»å…ƒå±‚
        self.lif_layer = DH_LIFNode(
            config['hidden_size'],
            tau_m_range,
            config['v_threshold'],
            self.device
        )

        # è¯»å‡ºå±‚
        self.readout = ReadoutIntegrator(
            config['hidden_size'],
            20,  # SHDæœ‰20ä¸ªç±»åˆ«
            tau_m_range,
            self.device
        )

        # åˆå§‹åŒ–æƒé‡
        torch.nn.init.xavier_normal_(self.readout.dense.weight)
        torch.nn.init.constant_(self.readout.dense.bias, 0)
        
        print("âœ… æ™®é€šSFNNæ¨¡å‹åˆ›å»ºå®Œæˆ")

    def forward(self, input_data):
        """
        å‰å‘ä¼ æ’­ - å®Œå…¨æŒ‰ç…§åŸè®ºæ–‡Dense_test_1layerå®ç°
        
        å‚æ•°:
            input_data: è¾“å…¥è„‰å†²åºåˆ—ï¼Œå½¢çŠ¶ä¸º[æ‰¹æ¬¡, æ—¶é—´æ­¥, ç‰¹å¾]
            
        è¿”å›:
            output: ç´¯ç§¯çš„softmaxè¾“å‡º
        """
        batch_size, seq_length, input_dim = input_data.shape

        # è®¾ç½®ç¥ç»å…ƒçŠ¶æ€
        self.lif_layer.set_neuron_state(batch_size)
        self.readout.set_neuron_state(batch_size)

        output = 0
        for i in range(seq_length):
            input_x = input_data[:, i, :].reshape(batch_size, input_dim)

            # çº¿æ€§å˜æ¢
            d_input = self.dense(input_x.float())

            # LIFå±‚
            mem_layer1, spike_layer1 = self.lif_layer.forward(d_input)

            # è¯»å‡ºå±‚
            mem_layer2 = self.readout.forward(spike_layer1)

            # ç´¯ç§¯è¾“å‡º - æŒ‰ç…§åŸè®ºæ–‡ï¼Œè·³è¿‡å‰10ä¸ªæ—¶é—´æ­¥
            if i > 10:
                output += F.softmax(mem_layer2, dim=1)

        return output

class DH_SFNN(nn.Module):
    """
    æ ‘çªå¼‚è´¨æ€§è„‰å†²å‰é¦ˆç¥ç»ç½‘ç»œ
    ç­‰ä»·äºåŸè®ºæ–‡çš„DH-SFNNå®ç°
    """

    def __init__(self, config, tau_m_range=(0.0, 4.0), tau_n_range=(2.0, 6.0)):
        """
        åˆå§‹åŒ–DH-SFNN
        
        å‚æ•°:
            config: å®éªŒé…ç½®
            tau_m_range: è†œç”µä½æ—¶é—´å¸¸æ•°èŒƒå›´
            tau_n_range: æ ‘çªæ—¶é—´å¸¸æ•°èŒƒå›´
        """
        super().__init__()
        self.config = config
        self.device = config['device']

        print(f"ğŸ—ï¸  åˆ›å»ºDH-SFNNæ¨¡å‹:")
        print(f"   éšè—å±‚å¤§å°: {config['hidden_size']}")
        print(f"   tau_mèŒƒå›´: {tau_m_range}")
        print(f"   tau_nèŒƒå›´: {tau_n_range}")

        # æ ‘çªå¼‚è´¨æ€§å±‚
        self.dh_layer = DH_DendriticLayer(
            700,  # SHDè¾“å…¥ç»´åº¦
            config['hidden_size'],
            tau_m_range,
            tau_n_range,
            4,  # æ ‘çªåˆ†æ”¯æ•°é‡
            config['v_threshold'],
            self.device
        )

        # è¯»å‡ºå±‚
        self.readout = ReadoutIntegrator(
            config['hidden_size'],
            20,  # SHDæœ‰20ä¸ªç±»åˆ«
            tau_m_range,
            self.device
        )

        # åˆå§‹åŒ–æƒé‡
        torch.nn.init.xavier_normal_(self.readout.dense.weight)
        torch.nn.init.constant_(self.readout.dense.bias, 0)
        
        print("âœ… DH-SFNNæ¨¡å‹åˆ›å»ºå®Œæˆ")

    def forward(self, input_data):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            input_data: è¾“å…¥è„‰å†²åºåˆ—ï¼Œå½¢çŠ¶ä¸º[æ‰¹æ¬¡, æ—¶é—´æ­¥, ç‰¹å¾]
            
        è¿”å›:
            output: ç´¯ç§¯çš„softmaxè¾“å‡º
        """
        batch_size, seq_length, input_dim = input_data.shape

        # è®¾ç½®ç¥ç»å…ƒçŠ¶æ€
        self.dh_layer.set_neuron_state(batch_size)
        self.readout.set_neuron_state(batch_size)

        output = 0
        for i in range(seq_length):
            input_x = input_data[:, i, :].reshape(batch_size, input_dim)

            # åº”ç”¨è¿æ¥æ©ç 
            self.dh_layer.apply_mask()

            # æ ‘çªå¼‚è´¨æ€§å±‚
            mem_layer1, spike_layer1 = self.dh_layer.forward(input_x)

            # è¯»å‡ºå±‚
            mem_layer2 = self.readout.forward(spike_layer1)

            # ç´¯ç§¯è¾“å‡º - è·³è¿‡å‰10ä¸ªæ—¶é—´æ­¥
            if i > 10:
                output += F.softmax(mem_layer2, dim=1)

        return output

# ==================== æ•°æ®å¤„ç† ====================

def convert_to_spike_tensor(times, units, dt=1e-3, max_time=1.0):
    """
    å°†è„‰å†²äº‹ä»¶è½¬æ¢ä¸ºå¯†é›†å¼ é‡
    
    å‚æ•°:
        times: è„‰å†²æ—¶é—´æ•°ç»„
        units: ç¥ç»å…ƒå•å…ƒæ•°ç»„
        dt: æ—¶é—´æ­¥é•¿
        max_time: æœ€å¤§æ—¶é—´
        
    è¿”å›:
        tensor: è„‰å†²å¼ é‡ï¼Œå½¢çŠ¶ä¸º[æ—¶é—´æ­¥, ç¥ç»å…ƒæ•°]
    """
    num_steps = int(max_time / dt)
    tensor = torch.zeros(num_steps, 700)
    
    # å°†æ—¶é—´è½¬æ¢ä¸ºæ—¶é—´æ­¥ç´¢å¼•
    time_indices = (times / dt).astype(int)
    
    # è¿‡æ»¤æœ‰æ•ˆçš„è„‰å†²äº‹ä»¶
    valid_mask = (time_indices < num_steps) & (units >= 1) & (units <= 700)
    
    if np.any(valid_mask):
        valid_times = time_indices[valid_mask]
        valid_units = units[valid_mask] - 1  # è½¬æ¢ä¸º0ç´¢å¼•
        
        # è®¾ç½®è„‰å†²
        tensor[valid_times, valid_units] = 1.0
    
    return tensor

def load_mock_data(num_train=2000, num_test=500):
    """
    åˆ›å»ºæ¨¡æ‹ŸSHDæ•°æ®ç”¨äºæµ‹è¯•
    
    å‚æ•°:
        num_train: è®­ç»ƒæ ·æœ¬æ•°
        num_test: æµ‹è¯•æ ·æœ¬æ•°
        
    è¿”å›:
        train_data, train_labels, test_data, test_labels: è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    """
    print(f"ğŸ² åˆ›å»ºæ¨¡æ‹ŸSHDæ•°æ®: è®­ç»ƒ{num_train}, æµ‹è¯•{num_test}")
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    train_data = torch.zeros(num_train, 1000, 700)
    train_labels = torch.randint(0, 20, (num_train,))
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = torch.zeros(num_test, 1000, 700)
    test_labels = torch.randint(0, 20, (num_test,))
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ ç±»åˆ«ç›¸å…³çš„è„‰å†²æ¨¡å¼
    for i in range(num_train):
        label = train_labels[i].item()
        # ä¸ºä¸åŒæ•°å­—åˆ›å»ºä¸åŒçš„è„‰å†²æ¨¡å¼
        num_spikes = 200 + label * 10  # ä¸åŒæ•°å­—æœ‰ä¸åŒçš„è„‰å†²å¯†åº¦
        spike_times = torch.randint(0, 1000, (num_spikes,))
        spike_neurons = torch.randint(label * 35, (label + 1) * 35, (num_spikes,))  # ç‰¹å®šç¥ç»å…ƒåŒºåŸŸ
        
        for t, n in zip(spike_times, spike_neurons):
            if n < 700:
                train_data[i, t, n] = 1.0
    
    for i in range(num_test):
        label = test_labels[i].item()
        num_spikes = 200 + label * 10
        spike_times = torch.randint(0, 1000, (num_spikes,))
        spike_neurons = torch.randint(label * 35, (label + 1) * 35, (num_spikes,))
        
        for t, n in zip(spike_times, spike_neurons):
            if n < 700:
                test_data[i, t, n] = 1.0
    
    print(f"âœ… æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ")
    return train_data, train_labels, test_data, test_labels

# ==================== è®­ç»ƒå‡½æ•° ====================

def train_shd_model(model, train_data, train_labels, test_data, test_labels, config, model_name):
    """
    è®­ç»ƒSHDæ¨¡å‹ - å®Œå…¨æŒ‰ç…§åŸè®ºæ–‡æ–¹å¼
    
    å‚æ•°:
        model: å¾…è®­ç»ƒçš„æ¨¡å‹
        train_data: è®­ç»ƒæ•°æ®
        train_labels: è®­ç»ƒæ ‡ç­¾
        test_data: æµ‹è¯•æ•°æ®
        test_labels: æµ‹è¯•æ ‡ç­¾
        config: å®éªŒé…ç½®
        model_name: æ¨¡å‹åç§°
        
    è¿”å›:
        best_acc: æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡
    """
    print(f"ğŸ‹ï¸  è®­ç»ƒ{model_name}")

    device = config['device']
    model = model.to(device)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)
    test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=config['batch_size'], shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=config['batch_size'], shuffle=False
    )

    criterion = nn.CrossEntropyLoss()

    # åˆ†ç»„ä¼˜åŒ–å™¨ - æŒ‰ç…§åŸè®ºæ–‡ï¼Œæ—¶é—´å¸¸æ•°ä½¿ç”¨2å€å­¦ä¹ ç‡
    base_params = []
    tau_m_params = []
    tau_n_params = []

    for name, param in model.named_parameters():
        if 'tau_m' in name:
            tau_m_params.append(param)
        elif 'tau_n' in name:
            tau_n_params.append(param)
        else:
            base_params.append(param)

    optimizer = torch.optim.Adam([
        {'params': base_params, 'lr': config['learning_rate']},
        {'params': tau_m_params, 'lr': config['learning_rate'] * 2},
        {'params': tau_n_params, 'lr': config['learning_rate'] * 2},
    ], lr=config['learning_rate'])

    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

    best_acc = 0.0

    for epoch in range(config['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_acc = 0
        sum_sample = 0

        for images, labels in train_loader:
            images = images.to(device)
            labels = labels.to(device)

            # åº”ç”¨æ©ç  (å¯¹DH-SFNN)
            if hasattr(model, 'dh_layer'):
                model.dh_layer.apply_mask()

            optimizer.zero_grad()
            predictions = model(images)
            loss = criterion(predictions, labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()

            # å†æ¬¡åº”ç”¨æ©ç 
            if hasattr(model, 'dh_layer'):
                model.dh_layer.apply_mask()

            _, predicted = torch.max(predictions.data, 1)
            train_acc += (predicted.cpu() == labels.cpu()).sum().item()
            sum_sample += labels.size(0)

        scheduler.step()
        train_acc = train_acc / sum_sample * 100

        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_acc = 0
        test_sum_sample = 0

        with torch.no_grad():
            for images, labels in test_loader:
                if hasattr(model, 'dh_layer'):
                    model.dh_layer.apply_mask()

                images = images.to(device)
                labels = labels.to(device)

                predictions = model(images)
                _, predicted = torch.max(predictions.data, 1)

                test_acc += (predicted.cpu() == labels.cpu()).sum().item()
                test_sum_sample += labels.size(0)

        test_acc = test_acc / test_sum_sample * 100

        if test_acc > best_acc:
            best_acc = test_acc

        # æ‰“å°è¿›åº¦
        if epoch % 20 == 0 or epoch == config['epochs'] - 1:
            print(f"   è½®æ¬¡ {epoch+1:2d}: è®­ç»ƒå‡†ç¡®ç‡={train_acc:5.1f}%, æµ‹è¯•å‡†ç¡®ç‡={test_acc:5.1f}%, æœ€ä½³={best_acc:5.1f}%")

    return best_acc

# ==================== ä¸»å®éªŒå‡½æ•° ====================

def run_shd_experiment():
    """è¿è¡ŒSHDå®éªŒ"""
    
    print("=" * 80)
    print("ğŸ”¢ DH-SNN SHDè„‰å†²æ•°å­—è¯†åˆ«å®éªŒ")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    setup_seed(42)
    
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {CONFIG['device']}")

    try:
        # åŠ è½½æ•°æ®ï¼ˆè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥åŠ è½½çœŸå®SHDæ•°æ®ï¼‰
        train_data, train_labels, test_data, test_labels = load_mock_data(2000, 500)

        results = {}

        # æµ‹è¯•ä¸åŒçš„æ—¶é—´å› å­é…ç½®
        for timing_name, timing_config in TIMING_CONFIGS.items():
            print(f"\nğŸ“Š æµ‹è¯• {timing_name} æ—¶é—´å› å­é…ç½®")
            print(f"   tau_m: {timing_config['tau_m']}, tau_n: {timing_config['tau_n']}")

            # è®­ç»ƒæ™®é€šSFNN
            print(f"\nğŸ”¬ è®­ç»ƒæ™®é€šSFNN ({timing_name})")
            vanilla_model = VanillaSFNN(CONFIG, tau_m_range=timing_config['tau_m'])
            vanilla_acc = train_shd_model(
                vanilla_model, train_data, train_labels, test_data, test_labels,
                CONFIG, f"æ™®é€šSFNN ({timing_name})"
            )

            # è®­ç»ƒDH-SFNN
            print(f"\nğŸ”¬ è®­ç»ƒDH-SFNN ({timing_name})")
            dh_model = DH_SFNN(
                CONFIG,
                tau_m_range=timing_config['tau_m'],
                tau_n_range=timing_config['tau_n']
            )
            dh_acc = train_shd_model(
                dh_model, train_data, train_labels, test_data, test_labels,
                CONFIG, f"DH-SFNN ({timing_name})"
            )

            # ä¿å­˜ç»“æœ
            results[timing_name] = {
                'vanilla': vanilla_acc,
                'dh_snn': dh_acc,
                'improvement': dh_acc - vanilla_acc
            }

            print(f"\nğŸ“ˆ {timing_name} é…ç½®ç»“æœ:")
            print(f"   æ™®é€šSFNN: {vanilla_acc:.1f}%")
            print(f"   DH-SFNN:  {dh_acc:.1f}%")
            print(f"   æ€§èƒ½æå‡: {dh_acc - vanilla_acc:+.1f} ä¸ªç™¾åˆ†ç‚¹")

        # æ€»ç»“æ‰€æœ‰ç»“æœ
        print(f"\nğŸ‰ SHDå®éªŒå®Œæˆ!")
        print("=" * 60)
        print("ğŸ“Š ä¸‰ç§æ—¶é—´å› å­é…ç½®æ€§èƒ½å¯¹æ¯”:")
        print("=" * 60)
        
        for timing_name in TIMING_CONFIGS.keys():
            vanilla_acc = results[timing_name]['vanilla']
            dh_acc = results[timing_name]['dh_snn']
            improvement = results[timing_name]['improvement']
            print(f"{timing_name:6s}: æ™®é€šSFNN {vanilla_acc:5.1f}% â†’ DH-SFNN {dh_acc:5.1f}% (æå‡{improvement:+5.1f}%)")

        # ä¿å­˜ç»“æœ
        results_path = Path("results/shd_experiment_results.json")
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json_results = {
                'experiment_info': {
                    'name': 'SHDè„‰å†²æ•°å­—è¯†åˆ«å®éªŒ',
                    'framework': 'SpikingJelly + DH-SNN',
                    'date': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'device': str(CONFIG['device'])
                },
                'timing_configs': TIMING_CONFIGS,
                'results': results
            }
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

        # ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
        print(f"\nğŸ“ˆ ä¸è®ºæ–‡ç»“æœå¯¹æ¯”:")
        print(f"è®ºæ–‡æ™®é€šSNN:   ~74%")
        print(f"è®ºæ–‡DH-SNN:    ~80%")
        
        best_config = max(results.keys(), key=lambda k: results[k]['dh_snn'])
        best_improvement = results[best_config]['improvement']
        
        if best_improvement > 5:
            print("ğŸ‰ DH-SNNæ˜¾è‘—ä¼˜äºæ™®é€šSNN - ç¬¦åˆé¢„æœŸ!")
        elif best_improvement > 0:
            print("âœ… DH-SNNä¼˜äºæ™®é€šSNN")
        else:
            print("âš ï¸  ç»“æœéœ€è¦è¿›ä¸€æ­¥åˆ†æ")

        return results

    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = run_shd_experiment()
    if results:
        print(f"\nğŸ SHDå®éªŒæˆåŠŸå®Œæˆ!")
    else:
        print(f"\nâŒ SHDå®éªŒå¤±è´¥")