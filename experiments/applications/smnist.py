#!/usr/bin/env python3
"""
DH-SNN Sequential MNISTï¼ˆåºåˆ—MNISTï¼‰å®éªŒ
=====================================

åŸºäºSpikingJellyæ¡†æ¶çš„DH-SNN vs æ™®é€šSNNå¯¹æ¯”å®éªŒ
ä½¿ç”¨Sequential MNISTæ•°æ®é›†è¿›è¡Œåºåˆ—åˆ†ç±»ä»»åŠ¡

"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import time
import json
import math
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent.parent.parent
sys.path.append(str(current_dir))

# SpikingJellyå¯¼å…¥
from spikingjelly.activation_based import neuron, functional, layer, surrogate

# PyTorchæ•°æ®é›†å¯¼å…¥
import torchvision
import torchvision.transforms as transforms

from dh_snn.utils import setup_seed

print("ğŸš€ DH-SNN Sequential MNISTåºåˆ—åˆ†ç±»å®éªŒ")
print("="*60)

# å®éªŒå‚æ•°
BATCH_SIZE = 64
LEARNING_RATE = 1e-3
NUM_EPOCHS = 20
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Sequential MNISTå‚æ•°
SEQ_LENGTH = 784  # 28x28åƒç´ åºåˆ—åŒ–
INPUT_SIZE = 1    # æ¯ä¸ªæ—¶é—´æ­¥è¾“å…¥ä¸€ä¸ªåƒç´ å€¼
HIDDEN_SIZE = 128 # éšè—å±‚å¤§å°
OUTPUT_SIZE = 10  # 10ä¸ªæ•°å­—ç±»åˆ«
NUM_BRANCHES = 4  # DH-SNNåˆ†æ”¯æ•°é‡

# ==================== æ•°æ®å¤„ç† ====================

class SequentialMNIST(torch.utils.data.Dataset):
    """
    Sequential MNISTæ•°æ®é›†
    å°†MNISTå›¾åƒè½¬æ¢ä¸ºåƒç´ åºåˆ—
    """
    
    def __init__(self, mnist_dataset, encoding='rate', time_steps=784):
        """
        åˆå§‹åŒ–Sequential MNISTæ•°æ®é›†
        
        å‚æ•°:
            mnist_dataset: åŸå§‹MNISTæ•°æ®é›†
            encoding: ç¼–ç æ–¹å¼ ('rate' æˆ– 'temporal')
            time_steps: æ—¶é—´æ­¥æ•°
        """
        self.mnist_dataset = mnist_dataset
        self.encoding = encoding
        self.time_steps = time_steps
        
        print(f"   ç¼–ç æ–¹å¼: {encoding}")
        print(f"   æ—¶é—´æ­¥æ•°: {time_steps}")
        print(f"   æ•°æ®æ ·æœ¬: {len(mnist_dataset)}")
    
    def __len__(self):
        return len(self.mnist_dataset)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        è¿”å›:
            spike_seq: è„‰å†²åºåˆ— [æ—¶é—´æ­¥, ç‰¹å¾ç»´åº¦]
            label: æ ‡ç­¾
        """
        image, label = self.mnist_dataset[idx]
        
        # å°†å›¾åƒå±•å¹³ä¸ºåºåˆ—
        pixel_seq = image.view(-1)  # [784]
        
        if self.encoding == 'rate':
            # æ³Šæ¾ç‡ç¼–ç 
            spike_seq = torch.rand(self.time_steps, INPUT_SIZE) < pixel_seq.unsqueeze(1)
            spike_seq = spike_seq.float()
        elif self.encoding == 'temporal':
            # æ—¶é—´ç¼–ç ï¼šåƒç´ å€¼å†³å®šè„‰å†²æ—¶é—´
            spike_seq = torch.zeros(self.time_steps, INPUT_SIZE)
            for i, pixel_val in enumerate(pixel_seq):
                if pixel_val > 0.1:  # é˜ˆå€¼è¿‡æ»¤
                    # åƒç´ å€¼è¶Šå¤§ï¼Œè„‰å†²è¶Šæ—©å‡ºç°
                    spike_time = int((1 - pixel_val) * self.time_steps * 0.8)
                    if spike_time < self.time_steps:
                        spike_seq[spike_time, 0] = 1.0
        else:
            # ç›´æ¥åºåˆ—åŒ–
            spike_seq = pixel_seq.unsqueeze(1).repeat(1, INPUT_SIZE)
            spike_seq = spike_seq.view(self.time_steps, INPUT_SIZE)
        
        return spike_seq, label

def create_sequential_mnist_datasets():
    """
    åˆ›å»ºSequential MNISTæ•°æ®é›†
    
    è¿”å›:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    print("ğŸ“Š åˆ›å»ºSequential MNISTæ•°æ®é›†...")
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))  # MNISTæ ‡å‡†åŒ–
    ])
    
    # åŠ è½½MNISTæ•°æ®é›†
    try:
        train_mnist = torchvision.datasets.MNIST(
            root='./data/mnist', 
            train=True, 
            download=True, 
            transform=transform
        )
        
        test_mnist = torchvision.datasets.MNIST(
            root='./data/mnist', 
            train=False, 
            download=True, 
            transform=transform
        )
    except Exception as e:
        print(f"âš ï¸  ä¸‹è½½MNISTå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {e}")
        return create_mock_sequential_mnist()
    
    # è½¬æ¢ä¸ºSequential MNIST
    print("   è½¬æ¢ä¸ºSequential MNISTæ ¼å¼...")
    train_smnist = SequentialMNIST(train_mnist, encoding='rate')
    test_smnist = SequentialMNIST(test_mnist, encoding='rate')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = torch.utils.data.DataLoader(
        train_smnist, 
        batch_size=BATCH_SIZE, 
        shuffle=True,
        num_workers=2 if torch.cuda.is_available() else 0
    )
    
    test_loader = torch.utils.data.DataLoader(
        test_smnist, 
        batch_size=BATCH_SIZE, 
        shuffle=False,
        num_workers=2 if torch.cuda.is_available() else 0
    )
    
    print(f"âœ… Sequential MNISTæ•°æ®é›†åˆ›å»ºå®Œæˆ")
    print(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"   æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    return train_loader, test_loader

def create_mock_sequential_mnist():
    """
    åˆ›å»ºæ¨¡æ‹ŸSequential MNISTæ•°æ®ç”¨äºæµ‹è¯•
    
    è¿”å›:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
    """
    print("ğŸ² åˆ›å»ºæ¨¡æ‹ŸSequential MNISTæ•°æ®...")
    
    # æ¨¡æ‹Ÿå‚æ•°
    num_train = 5000
    num_test = 1000
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    train_data = torch.zeros(num_train, SEQ_LENGTH, INPUT_SIZE)
    train_labels = torch.randint(0, OUTPUT_SIZE, (num_train,))
    
    test_data = torch.zeros(num_test, SEQ_LENGTH, INPUT_SIZE)
    test_labels = torch.randint(0, OUTPUT_SIZE, (num_test,))
    
    # ä¸ºæ¯ä¸ªæ•°å­—ç±»åˆ«åˆ›å»ºä¸åŒçš„åºåˆ—æ¨¡å¼
    for i in range(num_train):
        label = train_labels[i].item()
        
        # ä¸åŒæ•°å­—æœ‰ä¸åŒçš„è„‰å†²æ¨¡å¼
        base_pattern = torch.zeros(SEQ_LENGTH)
        
        # æ•°å­—0: ç¯å½¢æ¨¡å¼
        if label == 0:
            for t in range(0, 200, 20):
                base_pattern[t:t+10] = 0.8
        # æ•°å­—1: å‚ç›´çº¿æ¨¡å¼
        elif label == 1:
            for t in range(100, 600, 50):
                base_pattern[t:t+5] = 0.9
        # æ•°å­—2: æ³¢æµªæ¨¡å¼
        elif label == 2:
            for t in range(SEQ_LENGTH):
                if t % 30 < 15:
                    base_pattern[t] = 0.7 * np.sin(t * 0.1)
        # å…¶ä»–æ•°å­—çš„æ¨¡å¼
        else:
            pattern_freq = (label + 1) * 10
            for t in range(0, SEQ_LENGTH, pattern_freq):
                length = min(label + 5, SEQ_LENGTH - t)
                base_pattern[t:t+length] = 0.6 + label * 0.05
        
        # æ·»åŠ å™ªå£°
        noise = torch.randn(SEQ_LENGTH) * 0.1
        sequence = torch.clamp(base_pattern + noise, 0, 1)
        
        # è½¬æ¢ä¸ºè„‰å†²åºåˆ—
        train_data[i, :, 0] = (torch.rand(SEQ_LENGTH) < sequence).float()
    
    # ä¸ºæµ‹è¯•æ•°æ®ç”Ÿæˆç±»ä¼¼æ¨¡å¼
    for i in range(num_test):
        label = test_labels[i].item()
        base_pattern = torch.zeros(SEQ_LENGTH)
        
        if label == 0:
            for t in range(0, 200, 20):
                base_pattern[t:t+10] = 0.8
        elif label == 1:
            for t in range(100, 600, 50):
                base_pattern[t:t+5] = 0.9
        elif label == 2:
            for t in range(SEQ_LENGTH):
                if t % 30 < 15:
                    base_pattern[t] = 0.7 * np.sin(t * 0.1)
        else:
            pattern_freq = (label + 1) * 10
            for t in range(0, SEQ_LENGTH, pattern_freq):
                length = min(label + 5, SEQ_LENGTH - t)
                base_pattern[t:t+length] = 0.6 + label * 0.05
        
        noise = torch.randn(SEQ_LENGTH) * 0.1
        sequence = torch.clamp(base_pattern + noise, 0, 1)
        test_data[i, :, 0] = (torch.rand(SEQ_LENGTH) < sequence).float()
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)
    test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, batch_size=BATCH_SIZE, shuffle=False
    )
    
    print(f"   æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®: {train_data.shape}")
    print(f"   æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®: {test_data.shape}")
    
    return train_loader, test_loader

# ==================== å¤šé«˜æ–¯æ›¿ä»£å‡½æ•° ====================

class MultiGaussianSurrogate(torch.autograd.Function):
    """
    å¤šé«˜æ–¯æ›¿ä»£å‡½æ•°
    æŒ‰ç…§åŸè®ºæ–‡å®ç°
    """

    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.gt(0).float()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()

        # åŸè®ºæ–‡å‚æ•°
        lens = 0.5
        scale = 6.0
        height = 0.15
        gamma = 0.5

        def gaussian(x, mu=0., sigma=0.5):
            return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma

        # MultiGaussianå…¬å¼
        temp = gaussian(input, mu=0., sigma=lens) * (1. + height) \
             - gaussian(input, mu=lens, sigma=scale * lens) * height \
             - gaussian(input, mu=-lens, sigma=scale * lens) * height

        return grad_input * temp.float() * gamma

multi_gaussian_surrogate = MultiGaussianSurrogate.apply

# ==================== æ¨¡å‹å®šä¹‰ ====================

class SequentialMNIST_DH_SNN(nn.Module):
    """
    ç”¨äºSequential MNISTçš„DH-SNNæ¨¡å‹
    """
    
    def __init__(self, input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, 
                 output_size=OUTPUT_SIZE, num_branches=NUM_BRANCHES):
        """
        åˆå§‹åŒ–Sequential MNIST DH-SNNæ¨¡å‹
        
        å‚æ•°:
            input_size: è¾“å…¥ç»´åº¦
            hidden_size: éšè—å±‚ç»´åº¦
            output_size: è¾“å‡ºç»´åº¦
            num_branches: æ ‘çªåˆ†æ”¯æ•°é‡
        """
        super(SequentialMNIST_DH_SNN, self).__init__()
        
        print(f"ğŸ—ï¸  åˆ›å»ºSequential MNIST DH-SNNæ¨¡å‹:")
        print(f"   è¾“å…¥ç»´åº¦: {input_size}")
        print(f"   éšè—ç»´åº¦: {hidden_size}")
        print(f"   è¾“å‡ºç»´åº¦: {output_size}")
        print(f"   åˆ†æ”¯æ•°é‡: {num_branches}")
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_branches = num_branches
        
        # åˆ†æ”¯çº¿æ€§å±‚
        self.branch_layers = nn.ModuleList()
        for i in range(num_branches):
            self.branch_layers.append(
                layer.Linear(input_size, hidden_size // num_branches, bias=False)
            )
        
        # å¯å­¦ä¹ çš„æ—¶é—´å¸¸æ•°å‚æ•°
        # tau_n: æ ‘çªæ—¶é—´å¸¸æ•°ï¼Œç”¨Largeåˆå§‹åŒ–(2,6)é€‚åˆé•¿åºåˆ—
        self.tau_n = nn.Parameter(torch.empty(num_branches, hidden_size).uniform_(2, 6))
        # tau_m: è†œç”µä½æ—¶é—´å¸¸æ•°ï¼Œç”¨Mediumåˆå§‹åŒ–(0,4)
        self.tau_m = nn.Parameter(torch.empty(hidden_size).uniform_(0, 4))
        
        # è¾“å‡ºå±‚
        self.output_layer = layer.Linear(hidden_size, output_size)
        
        # ç¥ç»å…ƒçŠ¶æ€
        self.dendritic_currents = None
        self.membrane_potential = None
        self.spike_output = None
        
        print("âœ… Sequential MNIST DH-SNNæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    def reset_states(self, batch_size):
        """é‡ç½®ç¥ç»å…ƒçŠ¶æ€"""
        self.dendritic_currents = [
            torch.zeros(batch_size, self.hidden_size // self.num_branches).to(DEVICE)
            for _ in range(self.num_branches)
        ]
        self.membrane_potential = torch.rand(batch_size, self.hidden_size).to(DEVICE)
        self.spike_output = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
    
    def surrogate_gradient(self, x):
        """ä½¿ç”¨å¤šé«˜æ–¯æ›¿ä»£å‡½æ•°"""
        return multi_gaussian_surrogate(x)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥è„‰å†²åºåˆ—ï¼Œå½¢çŠ¶ä¸º[æ‰¹æ¬¡, æ—¶é—´æ­¥, ç‰¹å¾]
            
        è¿”å›:
            output: è¾“å‡ºlogits
        """
        batch_size, seq_len, input_dim = x.shape
        
        # é‡ç½®çŠ¶æ€
        self.reset_states(batch_size)
        
        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :]  # [batch, input_size]
            
            # å¤„ç†å„ä¸ªåˆ†æ”¯
            branch_outputs = []
            for i in range(self.num_branches):
                # åˆ†æ”¯çº¿æ€§å˜æ¢
                branch_input = self.branch_layers[i](x_t)
                
                # æ ‘çªæ—¶é—´å¸¸æ•°æ›´æ–°
                beta = torch.sigmoid(self.tau_n[i])
                self.dendritic_currents[i] = (
                    beta * self.dendritic_currents[i] + 
                    (1 - beta) * branch_input
                )
                
                branch_outputs.append(self.dendritic_currents[i])
            
            # åˆå¹¶åˆ†æ”¯è¾“å‡º
            total_current = torch.cat(branch_outputs, dim=1)  # [batch, hidden_size]
            
            # è†œç”µä½æ›´æ–°
            alpha = torch.sigmoid(self.tau_m)
            v_th = 1.0
            
            self.membrane_potential = (
                alpha * self.membrane_potential + 
                (1 - alpha) * total_current - 
                v_th * self.spike_output
            )
            
            # è„‰å†²ç”Ÿæˆ
            spike_input = self.membrane_potential - v_th
            self.spike_output = self.surrogate_gradient(spike_input)
            
            outputs.append(self.spike_output)
        
        # æ—¶é—´ç»´åº¦ç§¯åˆ† - ä½¿ç”¨å1/3çš„è¾“å‡ºï¼ˆé•¿åºåˆ—å¤„ç†ï¼‰
        start_idx = seq_len * 2 // 3
        integrated_output = torch.stack(outputs[start_idx:], dim=1).sum(dim=1)
        
        # è¾“å‡ºå±‚
        final_output = self.output_layer(integrated_output)
        
        return final_output

class SequentialMNIST_Vanilla_SNN(nn.Module):
    """
    ç”¨äºSequential MNISTçš„æ™®é€šSNNæ¨¡å‹
    """
    
    def __init__(self, input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE):
        """
        åˆå§‹åŒ–Sequential MNISTæ™®é€šSNNæ¨¡å‹
        """
        super(SequentialMNIST_Vanilla_SNN, self).__init__()
        
        print(f"ğŸ—ï¸  åˆ›å»ºSequential MNISTæ™®é€šSNNæ¨¡å‹:")
        print(f"   è¾“å…¥ç»´åº¦: {input_size}")
        print(f"   éšè—ç»´åº¦: {hidden_size}")
        print(f"   è¾“å‡ºç»´åº¦: {output_size}")
        
        # ç¬¬ä¸€å±‚
        self.fc1 = layer.Linear(input_size, hidden_size)
        self.lif1 = neuron.LIFNode(
            tau=2.0,
            v_threshold=1.0,
            surrogate_function=surrogate.ATan(),
            step_mode='s'
        )
        
        # è¾“å‡ºå±‚
        self.fc2 = layer.Linear(hidden_size, output_size)
        self.lif2 = neuron.LIFNode(
            tau=2.0,
            v_threshold=1.0,
            surrogate_function=surrogate.ATan(),
            step_mode='s'
        )
        
        print("âœ… Sequential MNISTæ™®é€šSNNæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥è„‰å†²åºåˆ—ï¼Œå½¢çŠ¶ä¸º[æ‰¹æ¬¡, æ—¶é—´æ­¥, ç‰¹å¾]
            
        è¿”å›:
            output: è¾“å‡ºlogits
        """
        batch_size, seq_len, input_dim = x.shape
        
        # é‡ç½®ç¥ç»å…ƒçŠ¶æ€
        functional.reset_net(self)
        
        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :]  # [batch, input_size]
            
            # ç¬¬ä¸€å±‚
            h1 = self.fc1(x_t)
            s1 = self.lif1(h1)
            
            # è¾“å‡ºå±‚
            h2 = self.fc2(s1)
            s2 = self.lif2(h2)
            
            outputs.append(s2)
        
        # æ—¶é—´ç»´åº¦ç§¯åˆ†
        start_idx = seq_len * 2 // 3
        integrated_output = torch.stack(outputs[start_idx:], dim=1).sum(dim=1)
        
        return integrated_output

# ==================== è®­ç»ƒå’Œæµ‹è¯•å‡½æ•° ====================

def train_smnist_model(model, train_loader, test_loader, model_name, num_epochs=NUM_EPOCHS):
    """
    è®­ç»ƒSequential MNISTæ¨¡å‹
    
    å‚æ•°:
        model: å¾…è®­ç»ƒçš„æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        model_name: æ¨¡å‹åç§°
        num_epochs: è®­ç»ƒè½®æ•°
        
    è¿”å›:
        results: è®­ç»ƒç»“æœ
    """
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {model_name}")
    print("-" * 50)
    
    model = model.to(DEVICE)
    
    # ä¼˜åŒ–å™¨é…ç½®
    if isinstance(model, SequentialMNIST_DH_SNN):
        # DH-SNNä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡
        base_params = []
        tau_params = []
        
        for name, param in model.named_parameters():
            if 'tau_' in name:
                tau_params.append(param)
            else:
                base_params.append(param)
        
        optimizer = torch.optim.Adam([
            {'params': base_params, 'lr': LEARNING_RATE},
            {'params': tau_params, 'lr': LEARNING_RATE * 2},  # æ—¶é—´å¸¸æ•°ç”¨2å€å­¦ä¹ ç‡
        ])
    else:
        # æ™®é€šSNNä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)
    criterion = nn.CrossEntropyLoss()
    
    best_test_acc = 0.0
    train_losses = []
    test_accuracies = []
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_data, batch_labels in train_loader:
            batch_data, batch_labels = batch_data.to(DEVICE), batch_labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(batch_data)
            
            loss = criterion(outputs, batch_labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé‡è¦ï¼šé˜²æ­¢é•¿åºåˆ—æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
            
            optimizer.step()
            
            running_loss += loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs.data, 1)
            total += batch_labels.size(0)
            correct += (predicted == batch_labels).sum().item()
        
        scheduler.step()
        
        train_acc = 100 * correct / total
        train_loss = running_loss / len(train_loader)
        
        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_correct = 0
        test_total = 0
        test_loss = 0.0
        
        with torch.no_grad():
            for batch_data, batch_labels in test_loader:
                batch_data, batch_labels = batch_data.to(DEVICE), batch_labels.to(DEVICE)
                
                outputs = model(batch_data)
                loss = criterion(outputs, batch_labels)
                test_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                test_total += batch_labels.size(0)
                test_correct += (predicted == batch_labels).sum().item()
        
        test_acc = 100 * test_correct / test_total
        test_loss = test_loss / len(test_loader)
        
        train_losses.append(train_loss)
        test_accuracies.append(test_acc)
        
        if test_acc > best_test_acc:
            best_test_acc = test_acc
        
        # æ‰“å°è¿›åº¦
        if epoch % 5 == 0 or epoch == num_epochs - 1:
            print(f'è½®æ¬¡ [{epoch+1}/{num_epochs}]: è®­ç»ƒæŸå¤±={train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡={train_acc:.1f}%, æµ‹è¯•å‡†ç¡®ç‡={test_acc:.1f}%, æœ€ä½³={best_test_acc:.1f}%')
    
    return {
        'train_losses': train_losses,
        'test_accuracies': test_accuracies,
        'best_test_acc': best_test_acc,
        'final_test_acc': test_acc
    }

# ==================== ä¸»å®éªŒå‡½æ•° ====================

def run_smnist_experiment():
    """è¿è¡ŒSequential MNISTå®éªŒ"""
    
    print("=" * 80)
    print("ğŸ”¢ DH-SNN Sequential MNISTåºåˆ—åˆ†ç±»å®éªŒ")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    setup_seed(42)
    
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        print("ğŸ“Š å‡†å¤‡Sequential MNISTæ•°æ®é›†...")
        train_loader, test_loader = create_sequential_mnist_datasets()
        
        # åˆ›å»ºæ¨¡å‹
        print(f"\nğŸ—ï¸  åœ¨ {DEVICE} ä¸Šåˆå§‹åŒ–æ¨¡å‹...")
        
        dh_snn_model = SequentialMNIST_DH_SNN()
        vanilla_snn_model = SequentialMNIST_Vanilla_SNN()
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   DH-SNNå‚æ•°: {sum(p.numel() for p in dh_snn_model.parameters()):,}")
        print(f"   æ™®é€šSNNå‚æ•°: {sum(p.numel() for p in vanilla_snn_model.parameters()):,}")
        
        # å¼€å§‹è®­ç»ƒå®éªŒ
        print(f"\nğŸ”¬ å¼€å§‹è®­ç»ƒå®éªŒ...")
        
        # è®­ç»ƒDH-SNN
        dh_results = train_smnist_model(
            dh_snn_model, train_loader, test_loader, "DH-SNN", NUM_EPOCHS
        )
        
        # è®­ç»ƒæ™®é€šSNN
        vanilla_results = train_smnist_model(
            vanilla_snn_model, train_loader, test_loader, "æ™®é€šSNN", NUM_EPOCHS
        )
        
        # ç»“æœå¯¹æ¯”
        print("\n" + "=" * 80)
        print("ğŸ¯ Sequential MNISTå®éªŒç»“æœå¯¹æ¯”")
        print("=" * 80)
        
        print(f"DH-SNNç»“æœ:")
        print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {dh_results['best_test_acc']:.2f}%")
        print(f"  æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {dh_results['final_test_acc']:.2f}%")
        
        print(f"\næ™®é€šSNNç»“æœ:")
        print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {vanilla_results['best_test_acc']:.2f}%")
        print(f"  æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {vanilla_results['final_test_acc']:.2f}%")
        
        # è®¡ç®—æ”¹è¿›
        best_improvement = dh_results['best_test_acc'] - vanilla_results['best_test_acc']
        final_improvement = dh_results['final_test_acc'] - vanilla_results['final_test_acc']
        
        if vanilla_results['best_test_acc'] > 0:
            best_relative = (dh_results['best_test_acc'] / vanilla_results['best_test_acc'] - 1) * 100
        else:
            best_relative = 0
            
        if vanilla_results['final_test_acc'] > 0:
            final_relative = (dh_results['final_test_acc'] / vanilla_results['final_test_acc'] - 1) * 100
        else:
            final_relative = 0
        
        print(f"\nğŸ“ˆ æ€§èƒ½æ”¹è¿›:")
        print(f"  æœ€ä½³å‡†ç¡®ç‡: +{best_improvement:.2f}% (ç›¸å¯¹: +{best_relative:.1f}%)")
        print(f"  æœ€ç»ˆå‡†ç¡®ç‡: +{final_improvement:.2f}% (ç›¸å¯¹: +{final_relative:.1f}%)")
        
        # ä¿å­˜ç»“æœ
        results_path = Path("results/smnist_experiment_results.json")
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        all_results = {
            'experiment_info': {
                'name': 'Sequential MNISTåºåˆ—åˆ†ç±»å®éªŒ',
                'framework': 'SpikingJelly + DH-SNN',
                'date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'dataset': 'Sequential MNIST',
                'sequence_length': SEQ_LENGTH,
                'num_classes': OUTPUT_SIZE,
                'batch_size': BATCH_SIZE,
                'num_epochs': NUM_EPOCHS,
                'num_branches': NUM_BRANCHES,
                'device': str(DEVICE)
            },
            'dh_snn': dh_results,
            'vanilla_snn': vanilla_results,
            'comparison': {
                'best_accuracy_improvement': {
                    'absolute': best_improvement,
                    'relative_percent': best_relative
                },
                'final_accuracy_improvement': {
                    'absolute': final_improvement,
                    'relative_percent': final_relative
                }
            }
        }
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # ä¸è®ºæ–‡/åŸºå‡†ç»“æœå¯¹æ¯”
        print(f"\nğŸ“ˆ ä¸åŸºå‡†ç»“æœå¯¹æ¯”:")
        print(f"Sequential MNISTæ˜¯ç»å…¸çš„åºåˆ—å­¦ä¹ åŸºå‡†ä»»åŠ¡")
        print(f"DH-SNNåœ¨é•¿åºåˆ—å¤„ç†ä¸­å±•ç°ä¼˜åŠ¿")
        
        if best_improvement > 3:
            print("ğŸ‰ DH-SNNæ˜¾è‘—ä¼˜äºæ™®é€šSNN - åºåˆ—å¤„ç†èƒ½åŠ›çªå‡º!")
        elif best_improvement > 1:
            print("âœ… DH-SNNä¼˜äºæ™®é€šSNN")
        else:
            print("âš ï¸  ç»“æœéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
        
        return all_results
        
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = run_smnist_experiment()
    if results:
        print(f"\nğŸ Sequential MNISTå®éªŒæˆåŠŸå®Œæˆ!")
    else:
        print(f"\nâŒ Sequential MNISTå®éªŒå¤±è´¥")