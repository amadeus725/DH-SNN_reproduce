#!/usr/bin/env python3
"""
å¤šæ—¶é—´å°ºåº¦XORå®éªŒ
å®ç°Figure 4bçš„åˆ†æ”¯æ•°é‡å¯¹æ¯”å®éªŒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
from typing import Dict, List, Tuple, Optional
import time

# å¯¼å…¥ç»„ä»¶
# sys.path.append removed during restructure

from .data_generator import MultiTimescaleXORGenerator
from .models import TwoBranchDH_SFNN, MultiBranchDH_SFNN
from dh_snn.core.models import VanillaSFNN

class MultiTimescaleXORExperiment:
    """
    å¤šæ—¶é—´å°ºåº¦XORå®éªŒç±»
    å®ç°Figure 4bä¸­çš„å„ç§é…ç½®å¯¹æ¯”
    """
    
    def __init__(self, 
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 save_dir: str = 'outputs/multi_timescale_xor'):
        """
        åˆå§‹åŒ–å®éªŒ
        
        Args:
            device: è®¡ç®—è®¾å¤‡
            save_dir: ä¿å­˜ç›®å½•
        """
        self.device = device
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # å®éªŒé…ç½®
        self.config = {
            'input_size': 100,
            'hidden_size': 64,
            'output_size': 1,
            'learning_rate': 1e-3,
            'batch_size': 32,
            'epochs': 200,
            'v_threshold': 1.0,
            'num_trials': 10,  # é‡å¤å®éªŒæ¬¡æ•°
        }
        
        # æ•°æ®ç”Ÿæˆå™¨
        self.data_generator = MultiTimescaleXORGenerator(
            dt=1.0,
            total_time=1000,
            signal1_duration=100,
            signal2_duration=50,
            signal2_interval=100,
            num_signal2=5,
            input_size=self.config['input_size'],
            device=device
        )
        
        print(f"ğŸš€ å¤šæ—¶é—´å°ºåº¦XORå®éªŒåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"ğŸ’¾ ä¿å­˜ç›®å½•: {save_dir}")
    
    def create_model(self, 
                    model_type: str,
                    num_branches: int = 2,
                    timing_config: str = 'beneficial',
                    learnable_timing: bool = True) -> nn.Module:
        """
        åˆ›å»ºæ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('vanilla', 'two_branch', 'multi_branch')
            num_branches: åˆ†æ”¯æ•°é‡
            timing_config: æ—¶é—´å¸¸æ•°é…ç½® ('beneficial', 'small', 'large', 'medium')
            learnable_timing: æ—¶é—´å¸¸æ•°æ˜¯å¦å¯å­¦ä¹ 
            
        Returns:
            æ¨¡å‹å®ä¾‹
        """
        # æ—¶é—´å¸¸æ•°é…ç½®
        timing_ranges = {
            'small': (-4.0, 0.0),
            'medium': (0.0, 4.0),
            'large': (2.0, 6.0),
            'beneficial_branch1': (2.0, 6.0),  # å¤§æ—¶é—´å¸¸æ•°
            'beneficial_branch2': (-4.0, 0.0)  # å°æ—¶é—´å¸¸æ•°
        }
        
        if model_type == 'vanilla':
            # Vanilla SFNN
            model = VanillaSFNN(
                input_size=self.config['input_size'],
                hidden_size=self.config['hidden_size'],
                output_size=self.config['output_size'],
                tau_m_range=timing_ranges['medium'],
                v_threshold=self.config['v_threshold'],
                device=self.device
            )
            
        elif model_type == 'two_branch':
            # åŒåˆ†æ”¯DH-SFNN
            if timing_config == 'beneficial':
                tau_n_branch1_range = timing_ranges['beneficial_branch1']
                tau_n_branch2_range = timing_ranges['beneficial_branch2']
                beneficial_init = True
            else:
                tau_n_branch1_range = timing_ranges[timing_config]
                tau_n_branch2_range = timing_ranges[timing_config]
                beneficial_init = False
            
            model = TwoBranchDH_SFNN(
                input_size=self.config['input_size'],
                hidden_size=self.config['hidden_size'],
                output_size=self.config['output_size'],
                tau_m_range=timing_ranges['medium'],
                tau_n_branch1_range=tau_n_branch1_range,
                tau_n_branch2_range=tau_n_branch2_range,
                v_threshold=self.config['v_threshold'],
                beneficial_init=beneficial_init,
                learnable_timing=learnable_timing,
                device=self.device
            )
            
        elif model_type == 'multi_branch':
            # å¤šåˆ†æ”¯DH-SFNN
            model = MultiBranchDH_SFNN(
                input_size=self.config['input_size'],
                hidden_size=self.config['hidden_size'],
                output_size=self.config['output_size'],
                num_branches=num_branches,
                tau_m_range=timing_ranges['medium'],
                tau_n_range=timing_ranges[timing_config],
                v_threshold=self.config['v_threshold'],
                learnable_timing=learnable_timing,
                device=self.device
            )
            
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        return model.to(self.device)
    
    def train_model(self, 
                   model: nn.Module,
                   train_data: torch.Tensor,
                   train_targets: torch.Tensor,
                   test_data: torch.Tensor,
                   test_targets: torch.Tensor,
                   model_name: str) -> Dict:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            model: æ¨¡å‹
            train_data: è®­ç»ƒæ•°æ®
            train_targets: è®­ç»ƒç›®æ ‡
            test_data: æµ‹è¯•æ•°æ®
            test_targets: æµ‹è¯•ç›®æ ‡
            model_name: æ¨¡å‹åç§°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        print(f"ğŸ‹ï¸ è®­ç»ƒ {model_name}")
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=self.config['learning_rate'])
        criterion = nn.MSELoss()
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)
        
        # è®­ç»ƒå†å²
        train_losses = []
        train_accs = []
        test_accs = []
        best_test_acc = 0.0
        
        num_batches = len(train_data) // self.config['batch_size']
        
        for epoch in range(self.config['epochs']):
            model.train()
            epoch_loss = 0.0
            epoch_acc = 0.0
            
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = torch.randperm(len(train_data))
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * self.config['batch_size']
                end_idx = start_idx + self.config['batch_size']
                batch_indices = indices[start_idx:end_idx]
                
                batch_data = train_data[batch_indices].to(self.device)
                batch_targets = train_targets[batch_indices].to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                if isinstance(model, TwoBranchDH_SFNN):
                    # åŒåˆ†æ”¯æ¨¡å‹éœ€è¦åˆ†ç¦»çš„è¾“å…¥
                    _, _, branch1_data, branch2_data = self.data_generator.generate_dataset(
                        self.config['batch_size'], split_by_branch=True
                    )
                    batch_branch1 = branch1_data[batch_indices].to(self.device)
                    batch_branch2 = branch2_data[batch_indices].to(self.device)
                    outputs, _ = model(batch_data, batch_branch1, batch_branch2)
                else:
                    outputs, _ = model(batch_data)
                
                # è®¡ç®—æŸå¤±ï¼ˆåªåœ¨è¾“å‡ºæ—¶é—´ç‚¹ï¼‰
                loss = criterion(outputs, batch_targets)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡ï¼ˆåŸºäºXORé€»è¾‘ï¼‰
                with torch.no_grad():
                    pred_binary = (outputs > 0.5).float()
                    target_binary = (batch_targets > 0.5).float()
                    acc = (pred_binary == target_binary).float().mean()
                    epoch_acc += acc.item()
            
            scheduler.step()
            
            # å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_loss = epoch_loss / num_batches
            avg_acc = epoch_acc / num_batches * 100
            
            # æµ‹è¯•
            test_acc = self.evaluate_model(model, test_data, test_targets)
            
            train_losses.append(avg_loss)
            train_accs.append(avg_acc)
            test_accs.append(test_acc)
            
            if test_acc > best_test_acc:
                best_test_acc = test_acc
            
            if epoch % 20 == 0 or epoch == self.config['epochs'] - 1:
                print(f"  Epoch {epoch+1:3d}: Loss={avg_loss:.4f}, Train={avg_acc:.1f}%, Test={test_acc:.1f}%, Best={best_test_acc:.1f}%")
        
        return {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'test_accs': test_accs,
            'best_test_acc': best_test_acc,
            'final_test_acc': test_accs[-1]
        }
    
    def evaluate_model(self, 
                      model: nn.Module,
                      test_data: torch.Tensor,
                      test_targets: torch.Tensor) -> float:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            model: æ¨¡å‹
            test_data: æµ‹è¯•æ•°æ®
            test_targets: æµ‹è¯•ç›®æ ‡
            
        Returns:
            æµ‹è¯•å‡†ç¡®ç‡
        """
        model.eval()
        total_acc = 0.0
        num_batches = len(test_data) // self.config['batch_size']
        
        with torch.no_grad():
            for batch_idx in range(num_batches):
                start_idx = batch_idx * self.config['batch_size']
                end_idx = start_idx + self.config['batch_size']
                
                batch_data = test_data[start_idx:end_idx].to(self.device)
                batch_targets = test_targets[start_idx:end_idx].to(self.device)
                
                # å‰å‘ä¼ æ’­
                if isinstance(model, TwoBranchDH_SFNN):
                    # åŒåˆ†æ”¯æ¨¡å‹éœ€è¦åˆ†ç¦»çš„è¾“å…¥
                    _, _, branch1_data, branch2_data = self.data_generator.generate_dataset(
                        self.config['batch_size'], split_by_branch=True
                    )
                    outputs, _ = model(batch_data, branch1_data, branch2_data)
                else:
                    outputs, _ = model(batch_data)
                
                # è®¡ç®—å‡†ç¡®ç‡
                pred_binary = (outputs > 0.5).float()
                target_binary = (batch_targets > 0.5).float()
                acc = (pred_binary == target_binary).float().mean()
                total_acc += acc.item()
        
        return total_acc / num_batches * 100
    
    def run_branch_comparison_experiment(self) -> Dict:
        """
        è¿è¡Œåˆ†æ”¯æ•°é‡å¯¹æ¯”å®éªŒ (Figure 4b)
        
        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        print("\nğŸ§ª è¿è¡Œåˆ†æ”¯æ•°é‡å¯¹æ¯”å®éªŒ (Figure 4b)")
        
        # ç”Ÿæˆæ•°æ®é›†
        print("ğŸ“Š ç”Ÿæˆå¤šæ—¶é—´å°ºåº¦XORæ•°æ®é›†...")
        train_input, train_target, train_branch1, train_branch2 = self.data_generator.generate_dataset(
            num_samples=1000, split_by_branch=True
        )
        test_input, test_target, test_branch1, test_branch2 = self.data_generator.generate_dataset(
            num_samples=200, split_by_branch=True
        )
        
        # å®éªŒé…ç½®
        experiments = [
            ('Vanilla SFNN', 'vanilla', 0, 'medium', True),
            ('1-Branch DH-SFNN (Small)', 'multi_branch', 1, 'small', True),
            ('1-Branch DH-SFNN (Large)', 'multi_branch', 1, 'large', True),
            ('2-Branch DH-SFNN (Beneficial)', 'two_branch', 2, 'beneficial', True),
            ('2-Branch DH-SFNN (Fixed)', 'two_branch', 2, 'beneficial', False),
            ('4-Branch DH-SFNN', 'multi_branch', 4, 'large', True),
        ]
        
        results = {}
        
        for exp_name, model_type, num_branches, timing_config, learnable in experiments:
            print(f"\nğŸ”¬ å®éªŒ: {exp_name}")
            
            trial_results = []
            
            for trial in range(self.config['num_trials']):
                print(f"  è¯•éªŒ {trial+1}/{self.config['num_trials']}")
                
                # åˆ›å»ºæ¨¡å‹
                model = self.create_model(
                    model_type=model_type,
                    num_branches=num_branches,
                    timing_config=timing_config,
                    learnable_timing=learnable
                )
                
                # è®­ç»ƒæ¨¡å‹
                result = self.train_model(
                    model, train_input, train_target,
                    test_input, test_target, f"{exp_name}_trial_{trial+1}"
                )
                
                trial_results.append(result['best_test_acc'])
            
            # ç»Ÿè®¡ç»“æœ
            mean_acc = np.mean(trial_results)
            std_acc = np.std(trial_results)
            
            results[exp_name] = {
                'mean_accuracy': mean_acc,
                'std_accuracy': std_acc,
                'all_trials': trial_results,
                'model_type': model_type,
                'num_branches': num_branches,
                'timing_config': timing_config,
                'learnable_timing': learnable
            }
            
            print(f"  ğŸ“ˆ ç»“æœ: {mean_acc:.1f}% Â± {std_acc:.1f}%")
        
        # ä¿å­˜ç»“æœ
        torch.save(results, os.path.join(self.save_dir, 'branch_comparison_results.pth'))
        
        return results

# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    # åˆ›å»ºå®éªŒ
    experiment = MultiTimescaleXORExperiment()
    
    # è¿è¡Œåˆ†æ”¯å¯¹æ¯”å®éªŒ
    results = experiment.run_branch_comparison_experiment()
    
    # æ‰“å°æ€»ç»“
    print("\nğŸ‰ å®éªŒå®Œæˆï¼ç»“æœæ€»ç»“:")
    for exp_name, result in results.items():
        mean_acc = result['mean_accuracy']
        std_acc = result['std_accuracy']
        print(f"  {exp_name}: {mean_acc:.1f}% Â± {std_acc:.1f}%")
