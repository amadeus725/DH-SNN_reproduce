"""
åˆ›æ–°æ€§èƒä½“å¼‚è´¨æ€§å®éªŒå¥—ä»¶
Innovative Soma Heterogeneity Experiment Suite

è¿™ä¸ªæ¨¡å—åŒ…å«å¤šä¸ªåˆ›æ–°æ€§çš„èƒä½“å¼‚è´¨æ€§æµ‹è¯•å®éªŒï¼Œæ¢ç´¢è„‰å†²ç¥ç»ç½‘ç»œä¸­èƒä½“å‚æ•°å¤šæ ·æ€§çš„æ·±å±‚æœºåˆ¶ã€‚

ä½œè€…: DH-SNN Reproduction Team
æ—¥æœŸ: 2025-06-14
ç‰ˆæœ¬: Ultimate
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from spikingjelly.clock_driven import neuron, functional
from typing import Tuple, Dict, List, Optional
import random
from datetime import datetime
from tqdm import tqdm
import json
import os

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
import sys
sys.path.append('..')
from dh_snn.models.heterogeneous_neurons import SomaHeterogeneousLIF


class AdvancedSomaHeterogeneousLIF(SomaHeterogeneousLIF):
    """
    é«˜çº§èƒä½“å¼‚è´¨æ€§LIFç¥ç»å…ƒ - åŒ…å«æ›´å¤šåˆ›æ–°ç‰¹æ€§
    Advanced Soma Heterogeneous LIF Neuron with Innovation Features
    """
    
    def __init__(self, 
                 n_neurons: int,
                 tau_range: Tuple[float, float] = (2.0, 20.0),
                 v_th_range: Tuple[float, float] = (0.8, 1.2),
                 v_reset_range: Tuple[float, float] = (-0.2, 0.0),
                 adaptation_ratio: float = 0.3,
                 noise_level: float = 0.01,
                 homeostasis: bool = True,
                 plasticity: bool = True,
                 surrogate_function=None,
                 **kwargs):
        
        super().__init__(n_neurons, tau_range, v_th_range, v_reset_range, 
                        adaptation_ratio, surrogate_function, **kwargs)
        
        # åˆ›æ–°ç‰¹æ€§1: å™ªå£°å¼‚è´¨æ€§
        self.noise_level = noise_level
        self.noise_scales = torch.rand(n_neurons) * noise_level
        
        # åˆ›æ–°ç‰¹æ€§2: ç¨³æ€è°ƒèŠ‚ (homeostasis)
        self.homeostasis = homeostasis
        if homeostasis:
            self.target_rate = torch.ones(n_neurons) * 0.1  # ç›®æ ‡å‘æ”¾ç‡ 10Hz
            self.rate_window = 100  # æ»‘åŠ¨çª—å£å¤§å°
            self.spike_history = torch.zeros(n_neurons, self.rate_window)
            self.history_pointer = 0
            self.homeostasis_strength = 0.001
        
        # åˆ›æ–°ç‰¹æ€§3: çªè§¦å¯å¡‘æ€§
        self.plasticity = plasticity
        if plasticity:
            self.stdp_lr = 0.01
            self.stdp_tau = 20.0
            self.pre_trace = torch.zeros(n_neurons)
            self.post_trace = torch.zeros(n_neurons)
        
        # åˆ›æ–°ç‰¹æ€§4: è®°å¿†çŠ¶æ€è¿½è¸ª
        self.spike_count = torch.zeros(n_neurons)
        self.total_time_steps = 0
        
        # æ³¨å†Œæ–°çš„ç¼“å†²åŒº
        self.register_buffer('noise_scales_buf', self.noise_scales)
        self.register_buffer('spike_count_buf', self.spike_count)
        if homeostasis:
            self.register_buffer('target_rate_buf', self.target_rate)
            self.register_buffer('spike_history_buf', self.spike_history)
        if plasticity:
            self.register_buffer('pre_trace_buf', self.pre_trace)
            self.register_buffer('post_trace_buf', self.post_trace)
    
    def add_heterogeneous_noise(self, x: torch.Tensor) -> torch.Tensor:
        """æ·»åŠ å¼‚è´¨æ€§å™ªå£°"""
        if self.noise_level > 0:
            noise_scales = self.noise_scales_buf.to(x.device)
            noise = torch.randn_like(x) * noise_scales
            return x + noise
        return x
    
    def neuronal_charge(self, x: torch.Tensor):
        """
        é‡å†™ç¥ç»å…ƒå……ç”µè¿‡ç¨‹ï¼Œå¤„ç†ç»´åº¦é—®é¢˜
        """
        # æ£€æŸ¥self.væ˜¯å¦éœ€è¦åˆå§‹åŒ–ï¼ˆå¯èƒ½æ˜¯Noneæˆ–floatï¼‰
        if self.v is None or not isinstance(self.v, torch.Tensor):
            if x.dim() == 3:  # [time_steps, batch_size, neurons]
                self.v = torch.zeros(x.shape[1], x.shape[2], device=x.device)  # [batch_size, neurons]
            else:  # [batch_size, neurons]
                self.v = torch.zeros_like(x.data)
        
        # åˆå§‹åŒ–v_th_adapted
        if self.v_th_adapted is None or not isinstance(self.v_th_adapted, torch.Tensor):
            if self.v.dim() > 1:  # [batch_size, neurons]
                self.v_th_adapted = self.v_th_buf.unsqueeze(0).expand_as(self.v)
            else:  # [neurons]
                self.v_th_adapted = self.v_th_buf.clone()
        
        # ä½¿ç”¨å¼‚è´¨æ€§è†œæ—¶é—´å¸¸æ•°
        tau_m = self.tau_m_buf.to(x.device)
        decay_factor = 1.0 / tau_m
        
        # ç¡®ä¿decay_factorçš„ç»´åº¦ä¸è¾“å…¥åŒ¹é…
        if x.dim() > 1 and decay_factor.dim() == 1:
            decay_factor = decay_factor.unsqueeze(0).expand_as(x)
        
        # å¼‚è´¨æ€§è†œåŠ¨åŠ›å­¦: v[t] = v[t-1] * (1 - 1/Ï„) + x[t]
        self.v = self.v * (1.0 - decay_factor) + x
    
    def update_homeostasis(self, spike: torch.Tensor):
        """æ›´æ–°ç¨³æ€è°ƒèŠ‚æœºåˆ¶"""
        if not self.homeostasis:
            return
        
        # æ›´æ–°å‘æ”¾å†å² - å¯¹batchç»´åº¦å–å¹³å‡
        self.spike_history_buf[:, self.history_pointer] = spike.mean(dim=0)
        self.history_pointer = (self.history_pointer + 1) % self.rate_window
        
        # è®¡ç®—å½“å‰å‘æ”¾ç‡
        current_rate = self.spike_history_buf.mean(dim=1)
        target_rate = self.target_rate_buf.to(spike.device)
        
        # è°ƒæ•´é˜ˆå€¼ä»¥ç»´æŒç›®æ ‡å‘æ”¾ç‡
        rate_error = current_rate - target_rate
        threshold_adjustment = rate_error * self.homeostasis_strength
        
        # åº”ç”¨ç¨³æ€è°ƒèŠ‚ - ç¡®ä¿ç»´åº¦åŒ¹é…
        if threshold_adjustment.dim() != self.v_th_adapted.dim():
            if self.v_th_adapted.dim() > 1 and threshold_adjustment.dim() == 1:
                threshold_adjustment = threshold_adjustment.unsqueeze(0).expand_as(self.v_th_adapted)
        
        self.v_th_adapted = self.v_th_adapted + threshold_adjustment
        
        # é™åˆ¶é˜ˆå€¼èŒƒå›´
        self.v_th_adapted = torch.clamp(self.v_th_adapted, 0.5, 2.0)
    
    def update_plasticity(self, x: torch.Tensor, spike: torch.Tensor):
        """æ›´æ–°çªè§¦å¯å¡‘æ€§"""
        if not self.plasticity:
            return
        
        # æ›´æ–°è¿¹ - å¯¹batchç»´åº¦å–å¹³å‡
        decay_factor = 1.0 - 1.0/self.stdp_tau
        self.pre_trace_buf = self.pre_trace_buf * decay_factor + x.mean(dim=0)
        self.post_trace_buf = self.post_trace_buf * decay_factor + spike.mean(dim=0)
    
    def neuronal_fire(self) -> torch.Tensor:
        """
        é‡å†™å‘æ”¾è¿‡ç¨‹ï¼Œå¤„ç†ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        """
        # æ£€æµ‹æ‰¹æ¬¡å¤§å°å˜åŒ–å¹¶é‡æ–°åˆå§‹åŒ–v_th_adapted
        current_batch_size = self.v.shape[0] if self.v.dim() > 1 else 1
        
        # å¦‚æœv_th_adaptedçš„æ‰¹æ¬¡å¤§å°ä¸å½“å‰ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ–
        if (self.v_th_adapted.dim() > 1 and 
            self.v_th_adapted.shape[0] != current_batch_size):
            if self.v.dim() > 1:  # [batch_size, neurons]
                self.v_th_adapted = self.v_th_buf.unsqueeze(0).expand(current_batch_size, -1).to(self.v.device)
            else:  # [neurons]
                self.v_th_adapted = self.v_th_buf.clone().to(self.v.device)
        
        # ä½¿ç”¨é€‚åº”æ€§é˜ˆå€¼
        v_th = self.v_th_adapted.to(self.v.device)
        
        # å¤„ç†ç»´åº¦ä¸åŒ¹é…ï¼šç¡®ä¿v_thä¸self.vçš„å½¢çŠ¶å…¼å®¹
        if self.v.dim() != v_th.dim():
            if self.v.dim() > 1 and v_th.dim() == 1:
                # self.v: [batch_size, neurons], v_th: [neurons]
                v_th = v_th.unsqueeze(0).expand_as(self.v)
            elif self.v.dim() == 1 and v_th.dim() > 1:
                # self.v: [neurons], v_th: [batch_size, neurons] or [some_dim, neurons]
                v_th = v_th[0] if v_th.shape[0] > 1 else v_th.squeeze(0)
        elif self.v.dim() == v_th.dim() == 2:
            # ä¸¤è€…éƒ½æ˜¯2Dï¼Œä½†batch sizeå¯èƒ½ä¸åŒ
            if self.v.shape[0] != v_th.shape[0]:
                if v_th.shape[0] == 1:
                    # v_th: [1, neurons], æ‰©å±•åˆ°åŒ¹é…self.vçš„batch size
                    v_th = v_th.expand_as(self.v)
                else:
                    # é‡æ–°åˆå§‹åŒ–v_th_adaptedä»¥åŒ¹é…å½“å‰batch size
                    self.v_th_adapted = self.v_th_buf.unsqueeze(0).expand(self.v.shape[0], -1).to(self.v.device)
                    v_th = self.v_th_adapted
        
        spike = (self.v >= v_th).float()
        
        # å¼‚è´¨æ€§é‡ç½®
        v_reset = self.v_reset_buf.to(self.v.device)
        if v_reset.dim() != spike.dim():
            if spike.dim() > 1 and v_reset.dim() == 1:
                v_reset = v_reset.unsqueeze(0).expand_as(spike)
            elif spike.dim() == 1 and v_reset.dim() > 1:
                v_reset = v_reset[0] if v_reset.shape[0] > 1 else v_reset.squeeze(0)
        
        self.v = torch.where(spike.bool(), v_reset, self.v)
        
        # é˜ˆå€¼é€‚åº” (åªå¯¹æœ‰é€‚åº”æ€§çš„ç¥ç»å…ƒ)
        adaptation_strength = self.adaptation_strength_buf.to(self.v.device)
        adaptation_mask = self.adaptation_mask_buf.to(self.v.device)
        
        # ç¡®ä¿adaptationç›¸å…³å‚æ•°çš„ç»´åº¦åŒ¹é…
        if spike.dim() > 1:
            if adaptation_strength.dim() == 1:
                adaptation_strength = adaptation_strength.unsqueeze(0).expand_as(spike)
            if adaptation_mask.dim() == 1:
                adaptation_mask = adaptation_mask.unsqueeze(0).expand_as(spike)
        
        # ç¡®ä¿adaptation_strengthä¸v_th_adaptedç»´åº¦åŒ¹é…
        if adaptation_strength.dim() != self.v_th_adapted.dim():
            if self.v_th_adapted.dim() > 1 and adaptation_strength.dim() == 1:
                adaptation_strength = adaptation_strength.unsqueeze(0).expand_as(self.v_th_adapted)
        
        self.v_th_adapted = torch.where(
            spike.bool() & adaptation_mask,
            self.v_th_adapted + adaptation_strength,  # å‘æ”¾åé˜ˆå€¼å‡é«˜
            self.v_th_adapted
        )
        
        return spike

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """é‡å†™å‰å‘ä¼ æ’­ï¼ŒåŒ…å«æ‰€æœ‰åˆ›æ–°ç‰¹æ€§"""
        # åˆå§‹åŒ–çŠ¶æ€ - ä¿®æ­£ç»´åº¦å¤„ç†
        if self.v is None or not isinstance(self.v, torch.Tensor):
            if x.dim() == 3:  # [time_steps, batch_size, neurons]
                self.v = torch.zeros(x.shape[1], x.shape[2], device=x.device)  # [batch_size, neurons]
            else:  # [batch_size, neurons]
                self.v = torch.zeros_like(x.data)
        elif self.v.shape[0] != x.shape[0]:
            # å¦‚æœbatch sizeæ”¹å˜äº†ï¼Œé‡æ–°åˆå§‹åŒ–v
            if x.dim() == 3:  # [time_steps, batch_size, neurons]
                self.v = torch.zeros(x.shape[1], x.shape[2], device=x.device)  # [batch_size, neurons]
            else:  # [batch_size, neurons]
                self.v = torch.zeros_like(x.data)
        
        if self.v_th_adapted is None or not isinstance(self.v_th_adapted, torch.Tensor):
            if self.v.dim() > 1:  # [batch_size, neurons]
                self.v_th_adapted = self.v_th_buf.unsqueeze(0).expand_as(self.v)
            else:  # [neurons]
                self.v_th_adapted = self.v_th_buf.clone()
        elif self.v_th_adapted.shape != self.v.shape:
            # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ–
            if self.v.dim() > 1:  # [batch_size, neurons]
                self.v_th_adapted = self.v_th_buf.unsqueeze(0).expand_as(self.v)
            else:  # [neurons]
                self.v_th_adapted = self.v_th_buf.clone()
        
        # æ·»åŠ å¼‚è´¨æ€§å™ªå£°
        x_noisy = self.add_heterogeneous_noise(x)
        
        # æ ‡å‡†LIFåŠ¨åŠ›å­¦
        self.neuronal_charge(x_noisy)
        spike = self.neuronal_fire()
        
        # æ›´æ–°è®¡æ•°å™¨ - å¯¹batchç»´åº¦æ±‚å’Œ
        if spike.dim() > 1:
            self.spike_count_buf += spike.sum(dim=0)  # å¯¹batchç»´åº¦æ±‚å’Œ
        else:
            self.spike_count_buf += spike
        self.total_time_steps += 1
        
        # åº”ç”¨åˆ›æ–°ç‰¹æ€§
        self.update_homeostasis(spike)
        self.update_plasticity(x_noisy, spike)
        
        return spike
    
    def get_heterogeneity_info(self) -> Dict:
        """è·å–å¼‚è´¨æ€§ä¿¡æ¯"""
        return {
            'tau_m': self.tau_m_buf.clone(),
            'v_th': self.v_th_buf.clone(),
            'v_reset': self.v_reset_buf.clone(),
            'adaptation_mask': self.adaptation_mask_buf.clone(),
            'adaptation_strength': self.adaptation_strength_buf.clone(),
            'noise_scales': self.noise_scales_buf.clone(),
            'spike_count': self.spike_count_buf.clone(),
            'current_rates': self.spike_count_buf / max(self.total_time_steps, 1)
        }
    
    def get_functional_analysis(self) -> Dict:
        """è·å–åŠŸèƒ½åˆ†æç»“æœ"""
        info = self.get_heterogeneity_info()
        
        # åˆ†æç¥ç»å…ƒåŠŸèƒ½åˆ†åŒ–
        tau_m = info['tau_m']
        v_th = info['v_th']
        rates = info['current_rates']
        
        # æŒ‰è†œæ—¶é—´å¸¸æ•°åˆ†ç»„
        fast_neurons = tau_m < tau_m.median()
        slow_neurons = tau_m >= tau_m.median()
        
        # æŒ‰é˜ˆå€¼åˆ†ç»„
        sensitive_neurons = v_th < v_th.median()
        insensitive_neurons = v_th >= v_th.median()
        
        return {
            'functional_groups': {
                'fast_sensitive': (fast_neurons & sensitive_neurons).sum().item(),
                'fast_insensitive': (fast_neurons & insensitive_neurons).sum().item(),
                'slow_sensitive': (slow_neurons & sensitive_neurons).sum().item(),
                'slow_insensitive': (slow_neurons & insensitive_neurons).sum().item()
            },
            'group_activities': {
                'fast_neurons_rate': rates[fast_neurons].mean().item(),
                'slow_neurons_rate': rates[slow_neurons].mean().item(),
                'sensitive_neurons_rate': rates[sensitive_neurons].mean().item(),
                'insensitive_neurons_rate': rates[insensitive_neurons].mean().item()
            },
            'diversity_metrics': {
                'tau_m_cv': (tau_m.std() / tau_m.mean()).item(),
                'v_th_cv': (v_th.std() / v_th.mean()).item(),
                'rate_cv': (rates.std() / (rates.mean() + 1e-8)).item()
            }
        }


class DynamicXORDataset:
    """
    åŠ¨æ€XORæ•°æ®é›† - åŒ…å«æ—¶å˜å¤æ‚åº¦
    Dynamic XOR Dataset with Time-varying Complexity
    """
    
    def __init__(self, 
                 time_steps: int = 200,
                 base_delay_ranges: List[Tuple[int, int]] = [(10, 20), (30, 50), (70, 100), (120, 180)],
                 noise_probability: float = 0.1,
                 interference_probability: float = 0.05):
        
        self.time_steps = time_steps
        self.base_delay_ranges = base_delay_ranges
        self.noise_probability = noise_probability
        self.interference_probability = interference_probability
    
    def generate_noisy_sample(self, delay_range: Tuple[int, int]) -> Tuple[torch.Tensor, int, Dict]:
        """ç”Ÿæˆå¸¦å™ªå£°çš„æ ·æœ¬"""
        data = torch.zeros(self.time_steps, 2)
        
        # åŸºç¡€XORé€»è¾‘
        delay_min, delay_max = delay_range
        delay = random.randint(delay_min, delay_max)
        
        # ç¬¬ä¸€ä¸ªè„‰å†²
        pulse1_time = random.randint(5, 15)
        input1_value = random.choice([0, 1])
        if input1_value:
            data[pulse1_time, 0] = 1.0
        
        # ç¬¬äºŒä¸ªè„‰å†²
        pulse2_time = pulse1_time + delay
        if pulse2_time < self.time_steps:
            input2_value = random.choice([0, 1])
            if input2_value:
                data[pulse2_time, 1] = 1.0
        else:
            input2_value = 0
        
        # æ·»åŠ éšæœºå™ªå£°è„‰å†²
        if random.random() < self.noise_probability:
            noise_times = random.sample(range(self.time_steps), k=random.randint(1, 3))
            for t in noise_times:
                if random.random() < 0.5:
                    data[t, random.randint(0, 1)] = random.uniform(0.3, 0.7)
        
        # æ·»åŠ å¹²æ‰°ä¿¡å·
        if random.random() < self.interference_probability:
            interference_start = random.randint(0, self.time_steps - 20)
            interference_duration = random.randint(5, 15)
            for t in range(interference_start, min(interference_start + interference_duration, self.time_steps)):
                data[t, :] += random.uniform(-0.2, 0.2)
        
        label = input1_value ^ input2_value
        
        metadata = {
            'delay': delay,
            'pulse1_time': pulse1_time,
            'pulse2_time': pulse2_time,
            'input1_value': input1_value,
            'input2_value': input2_value,
            'has_noise': random.random() < self.noise_probability,
            'has_interference': random.random() < self.interference_probability
        }
        
        return data, label, metadata
    
    def generate_hierarchical_batch(self, batch_size: int, complexity_level: int = 1) -> Tuple[torch.Tensor, torch.Tensor, List[Dict]]:
        """ç”Ÿæˆåˆ†å±‚å¤æ‚åº¦çš„æ‰¹æ¬¡"""
        batch_data = []
        batch_labels = []
        batch_metadata = []
        
        # æ ¹æ®å¤æ‚åº¦çº§åˆ«é€‰æ‹©delay ranges
        if complexity_level == 1:
            delay_ranges = self.base_delay_ranges[:1]  # åªç”¨æœ€çŸ­çš„
        elif complexity_level == 2:
            delay_ranges = self.base_delay_ranges[:2]  # çŸ­å’Œä¸­ç­‰
        elif complexity_level == 3:
            delay_ranges = self.base_delay_ranges[:3]  # çŸ­ã€ä¸­ã€é•¿
        else:
            delay_ranges = self.base_delay_ranges      # å…¨éƒ¨å¤æ‚åº¦
        
        for _ in range(batch_size):
            delay_range = random.choice(delay_ranges)
            data, label, metadata = self.generate_noisy_sample(delay_range)
            
            batch_data.append(data)
            batch_labels.append(label)
            batch_metadata.append(metadata)
        
        return (torch.stack(batch_data), 
                torch.tensor(batch_labels, dtype=torch.long),
                batch_metadata)


class InnovativeSomaNet(nn.Module):
    """
    åˆ›æ–°èƒä½“å¼‚è´¨æ€§ç½‘ç»œ
    Innovative Soma Heterogeneity Network
    """
    
    def __init__(self, 
                 input_size: int = 2,
                 hidden_sizes: List[int] = [64, 32],
                 output_size: int = 2,
                 heterogeneity_configs: Optional[List[Dict]] = None):
        super().__init__()
        
        # é»˜è®¤å¼‚è´¨æ€§é…ç½®
        if heterogeneity_configs is None:
            heterogeneity_configs = [
                {'tau_range': (2.0, 15.0), 'v_th_range': (0.7, 1.1), 'adaptation_ratio': 0.4, 'noise_level': 0.02},
                {'tau_range': (5.0, 25.0), 'v_th_range': (0.9, 1.3), 'adaptation_ratio': 0.2, 'noise_level': 0.01}
            ]
        
        self.layers = nn.ModuleList()
        prev_size = input_size
        
        # æ„å»ºå¼‚è´¨æ€§éšè—å±‚
        for i, (hidden_size, config) in enumerate(zip(hidden_sizes, heterogeneity_configs)):
            # çº¿æ€§å±‚
            self.layers.append(nn.Linear(prev_size, hidden_size))
            
            # å¼‚è´¨æ€§ç¥ç»å…ƒå±‚
            hetero_lif = AdvancedSomaHeterogeneousLIF(
                n_neurons=hidden_size,
                **config
            )
            self.layers.append(hetero_lif)
            
            prev_size = hidden_size
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(prev_size, output_size)
        self.output_lif = neuron.LIFNode(tau=10.0)
        
        # è®°å½•ç½‘ç»œæ´»åŠ¨
        self.layer_activities = []
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        batch_size, time_steps, _ = x.shape
        
        # é‡ç½®ç½‘ç»œçŠ¶æ€
        functional.reset_net(self)
        self.layer_activities = []
        
        outputs = []
        layer_activities = {i: [] for i in range(len(self.layers)//2)}
        
        for t in range(time_steps):
            current_input = x[:, t, :]
            
            # é€šè¿‡å„ä¸ªå¼‚è´¨æ€§å±‚
            for i in range(0, len(self.layers), 2):
                linear_layer = self.layers[i]
                lif_layer = self.layers[i+1]
                
                current_input = linear_layer(current_input)
                current_input = lif_layer(current_input)
                
                # è®°å½•å±‚æ´»åŠ¨
                layer_activities[i//2].append(current_input.sum().item())
            
            # è¾“å‡ºå±‚
            output = self.output_layer(current_input)
            output = self.output_lif(output)
            outputs.append(output)
        
        # ä¿å­˜æ´»åŠ¨è®°å½•
        self.layer_activities = layer_activities
        
        # ç´¯ç§¯è¾“å‡º
        return torch.stack(outputs, dim=1).sum(dim=1)
    
    def get_layer_analysis(self) -> Dict:
        """è·å–å±‚é—´åˆ†æ"""
        analysis = {}
        
        for layer_idx, (linear_layer, lif_layer) in enumerate(zip(self.layers[::2], self.layers[1::2])):
            if isinstance(lif_layer, AdvancedSomaHeterogeneousLIF):
                layer_info = lif_layer.get_functional_analysis()
                layer_info['total_activity'] = sum(self.layer_activities.get(layer_idx, []))
                analysis[f'layer_{layer_idx}'] = layer_info
        
        return analysis


def run_innovative_experiments():
    """è¿è¡Œåˆ›æ–°æ€§èƒä½“å¼‚è´¨æ€§å®éªŒ"""
    print("ğŸš€ å¼€å§‹åˆ›æ–°æ€§èƒä½“å¼‚è´¨æ€§å®éªŒ...")
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å®éªŒé…ç½®
    experiments = {
        'baseline': {
            'network': InnovativeSomaNet(
                hidden_sizes=[128],
                heterogeneity_configs=[{
                    'tau_range': (10.0, 10.0),  # æ— å¼‚è´¨æ€§
                    'v_th_range': (1.0, 1.0),
                    'adaptation_ratio': 0.0,
                    'noise_level': 0.0,
                    'homeostasis': False,
                    'plasticity': False
                }]
            ),
            'name': 'Baseline (No Heterogeneity)'
        },
        'basic_hetero': {
            'network': InnovativeSomaNet(
                hidden_sizes=[128],
                heterogeneity_configs=[{
                    'tau_range': (2.0, 20.0),
                    'v_th_range': (0.8, 1.2),
                    'adaptation_ratio': 0.3,
                    'noise_level': 0.0,
                    'homeostasis': False,
                    'plasticity': False
                }]
            ),
            'name': 'Basic Soma Heterogeneity'
        },
        'advanced_hetero': {
            'network': InnovativeSomaNet(
                hidden_sizes=[128],
                heterogeneity_configs=[{
                    'tau_range': (2.0, 20.0),
                    'v_th_range': (0.8, 1.2),
                    'adaptation_ratio': 0.3,
                    'noise_level': 0.02,
                    'homeostasis': True,
                    'plasticity': True
                }]
            ),
            'name': 'Advanced Soma Heterogeneity (with Homeostasis & Plasticity)'
        },
        'hierarchical_hetero': {
            'network': InnovativeSomaNet(
                hidden_sizes=[64, 32],
                heterogeneity_configs=[
                    {
                        'tau_range': (2.0, 10.0),   # å¿«é€Ÿå±‚
                        'v_th_range': (0.7, 1.0),
                        'adaptation_ratio': 0.5,
                        'noise_level': 0.03,
                        'homeostasis': True,
                        'plasticity': True
                    },
                    {
                        'tau_range': (15.0, 30.0),  # æ…¢é€Ÿå±‚
                        'v_th_range': (1.0, 1.4),
                        'adaptation_ratio': 0.2,
                        'noise_level': 0.01,
                        'homeostasis': True,
                        'plasticity': True
                    }
                ]
            ),
            'name': 'Hierarchical Soma Heterogeneity'
        }
    }
    
    # åˆ›å»ºåŠ¨æ€æ•°æ®é›†
    dataset = DynamicXORDataset(time_steps=150)
    
    results = {}
    
    for exp_name, exp_config in experiments.items():
        print(f"\nğŸ§ª è¿è¡Œå®éªŒ: {exp_config['name']}")
        
        network = exp_config['network'].to(device)
        optimizer = torch.optim.Adam(network.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒè¿‡ç¨‹
        network.train()
        train_losses = []
        
        for epoch in tqdm(range(20), desc=f"è®­ç»ƒ {exp_name}"):
            # ç”Ÿæˆè®­ç»ƒæ•°æ®
            batch_data, batch_labels, metadata = dataset.generate_hierarchical_batch(
                batch_size=32, 
                complexity_level=min(epoch//5 + 1, 4)  # é€æ¸å¢åŠ å¤æ‚åº¦
            )
            
            batch_data = batch_data.to(device)
            batch_labels = batch_labels.to(device)
            
            optimizer.zero_grad()
            outputs = network(batch_data)
            loss = criterion(outputs, batch_labels)
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
        
        # æµ‹è¯•ä¸åŒå¤æ‚åº¦
        network.eval()
        complexity_results = {}
        
        with torch.no_grad():
            for complexity in range(1, 5):
                test_data, test_labels, test_metadata = dataset.generate_hierarchical_batch(
                    batch_size=100, 
                    complexity_level=complexity
                )
                
                test_data = test_data.to(device)
                test_labels = test_labels.to(device)
                
                outputs = network(test_data)
                _, predicted = torch.max(outputs, 1)
                accuracy = (predicted == test_labels).float().mean().item()
                
                complexity_results[f'complexity_{complexity}'] = accuracy
        
        # è·å–ç½‘ç»œåˆ†æ
        network_analysis = network.get_layer_analysis()
        
        results[exp_name] = {
            'name': exp_config['name'],
            'final_loss': train_losses[-1],
            'complexity_results': complexity_results,
            'network_analysis': network_analysis,
            'train_losses': train_losses
        }
        
        print(f"âœ… {exp_config['name']} å®Œæˆ")
        print(f"   æœ€ç»ˆæŸå¤±: {train_losses[-1]:.4f}")
        for comp, acc in complexity_results.items():
            print(f"   {comp}: {acc:.3f}")
    
    return results


def visualize_results(results: Dict):
    """å¯è§†åŒ–å®éªŒç»“æœ"""
    print("\nğŸ“Š ç”Ÿæˆå®éªŒç»“æœå¯è§†åŒ–...")
    
    # è®¾ç½®ç»˜å›¾é£æ ¼
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. å¤æ‚åº¦-å‡†ç¡®ç‡æ›²çº¿
    ax1 = axes[0, 0]
    complexities = [1, 2, 3, 4]
    
    for exp_name, exp_result in results.items():
        accuracies = [exp_result['complexity_results'][f'complexity_{c}'] for c in complexities]
        ax1.plot(complexities, accuracies, marker='o', linewidth=2, label=exp_result['name'])
    
    ax1.set_xlabel('Task Complexity Level')
    ax1.set_ylabel('Accuracy')
    ax1.set_title('Performance vs Task Complexity')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. è®­ç»ƒæŸå¤±æ›²çº¿
    ax2 = axes[0, 1]
    for exp_name, exp_result in results.items():
        ax2.plot(exp_result['train_losses'], label=exp_result['name'], alpha=0.8)
    
    ax2.set_xlabel('Training Epoch')
    ax2.set_ylabel('Loss')
    ax2.set_title('Training Loss Curves')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. æ€§èƒ½é›·è¾¾å›¾
    ax3 = axes[1, 0]
    categories = ['Complexity 1', 'Complexity 2', 'Complexity 3', 'Complexity 4']
    
    # ä½¿ç”¨æåæ ‡å­å›¾æ¥åˆ›å»ºé›·è¾¾å›¾
    ax3.remove()  # ç§»é™¤åŸæœ‰å­å›¾
    ax3 = fig.add_subplot(2, 2, 3, projection='polar')
    
    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)
    
    for exp_name, exp_result in results.items():
        values = [exp_result['complexity_results'][f'complexity_{c}'] for c in complexities]
        
        # é—­åˆå›¾å½¢
        angles_closed = np.concatenate([angles, [angles[0]]])
        values_closed = values + [values[0]]
        
        ax3.plot(angles_closed, values_closed, marker='o', linewidth=2, label=exp_result['name'])
        ax3.fill(angles_closed, values_closed, alpha=0.1)
    
    ax3.set_thetagrids(angles * 180/np.pi, categories)
    ax3.set_ylim(0, 1)
    ax3.set_title('Performance Radar Chart', pad=20)
    ax3.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    ax3.grid(True)
    
    # 4. ç½‘ç»œåˆ†æçƒ­å›¾
    ax4 = axes[1, 1]
    
    # æå–å¤šæ ·æ€§æŒ‡æ ‡
    diversity_data = []
    exp_names = []
    
    for exp_name, exp_result in results.items():
        if 'network_analysis' in exp_result and exp_result['network_analysis']:
            layer_0 = exp_result['network_analysis'].get('layer_0', {})
            diversity_metrics = layer_0.get('diversity_metrics', {})
            
            if diversity_metrics:
                diversity_data.append([
                    diversity_metrics.get('tau_m_cv', 0),
                    diversity_metrics.get('v_th_cv', 0),
                    diversity_metrics.get('rate_cv', 0)
                ])
                exp_names.append(exp_result['name'][:15])  # æˆªçŸ­åç§°
    
    if diversity_data:
        diversity_array = np.array(diversity_data)
        im = ax4.imshow(diversity_array, cmap='viridis', aspect='auto')
        ax4.set_xticks(range(3))
        ax4.set_xticklabels(['Ï„_m CV', 'V_th CV', 'Rate CV'])
        ax4.set_yticks(range(len(exp_names)))
        ax4.set_yticklabels(exp_names)
        ax4.set_title('Diversity Metrics Heatmap')
        plt.colorbar(im, ax=ax4)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'innovative_soma_results_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ“ˆ ç»“æœå›¾è¡¨å·²ä¿å­˜: {filename}")
    
    plt.show()


def save_detailed_results(results: Dict):
    """ä¿å­˜è¯¦ç»†ç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'innovative_soma_detailed_results_{timestamp}.json'
    
    # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    serializable_results = {}
    for exp_name, exp_result in results.items():
        serializable_results[exp_name] = {
            'name': exp_result['name'],
            'final_loss': exp_result['final_loss'],
            'complexity_results': exp_result['complexity_results'],
            'train_losses': exp_result['train_losses']
        }
        
        # å¤„ç†ç½‘ç»œåˆ†ææ•°æ®
        if 'network_analysis' in exp_result:
            network_analysis = {}
            for layer_name, layer_data in exp_result['network_analysis'].items():
                layer_analysis = {}
                for key, value in layer_data.items():
                    if isinstance(value, dict):
                        layer_analysis[key] = value
                    else:
                        layer_analysis[key] = float(value) if isinstance(value, (int, float, np.number)) else str(value)
                network_analysis[layer_name] = layer_analysis
            serializable_results[exp_name]['network_analysis'] = network_analysis
    
    with open(filename, 'w') as f:
        json.dump(serializable_results, f, indent=2)
    
    print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {filename}")


def print_summary_report(results: Dict):
    """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ åˆ›æ–°æ€§èƒä½“å¼‚è´¨æ€§å®éªŒæ€»ç»“æŠ¥å‘Š")
    print("="*60)
    
    # æŒ‰æœ€é«˜å¤æ‚åº¦å‡†ç¡®ç‡æ’åº
    sorted_results = sorted(
        results.items(), 
        key=lambda x: x[1]['complexity_results']['complexity_4'], 
        reverse=True
    )
    
    print(f"\nğŸ† å®éªŒæ’å (æŒ‰æœ€é«˜å¤æ‚åº¦ä»»åŠ¡å‡†ç¡®ç‡):")
    for i, (exp_name, exp_result) in enumerate(sorted_results, 1):
        acc = exp_result['complexity_results']['complexity_4']
        print(f"{i}. {exp_result['name']}: {acc:.3f}")
    
    print(f"\nğŸ“Š è¯¦ç»†æ€§èƒ½å¯¹æ¯”:")
    print(f"{'å®éªŒåç§°':<35} {'ç®€å•':<8} {'ä¸­ç­‰':<8} {'å¤æ‚':<8} {'è¶…å¤æ‚':<8} {'å¹³å‡':<8}")
    print("-" * 75)
    
    for exp_name, exp_result in sorted_results:
        name = exp_result['name'][:32] + "..." if len(exp_result['name']) > 32 else exp_result['name']
        
        c1 = exp_result['complexity_results']['complexity_1']
        c2 = exp_result['complexity_results']['complexity_2']
        c3 = exp_result['complexity_results']['complexity_3']
        c4 = exp_result['complexity_results']['complexity_4']
        avg = (c1 + c2 + c3 + c4) / 4
        
        print(f"{name:<35} {c1:<8.3f} {c2:<8.3f} {c3:<8.3f} {c4:<8.3f} {avg:<8.3f}")
    
    print(f"\nğŸ” å…³é”®å‘ç°:")
    
    # æ‰¾åˆ°æœ€ä½³è¡¨ç°çš„å®éªŒ
    best_exp = sorted_results[0]
    best_name = best_exp[1]['name']
    best_acc = best_exp[1]['complexity_results']['complexity_4']
    
    print(f"1. æœ€ä½³å®éªŒé…ç½®: {best_name}")
    print(f"   æœ€é«˜å¤æ‚åº¦ä»»åŠ¡å‡†ç¡®ç‡: {best_acc:.3f}")
    
    # åˆ†æå¼‚è´¨æ€§æ•ˆæœ
    baseline_acc = results.get('baseline', {}).get('complexity_results', {}).get('complexity_4', 0)
    if baseline_acc > 0:
        improvement = (best_acc - baseline_acc) / baseline_acc * 100
        print(f"2. ç›¸æ¯”åŸºçº¿æå‡: {improvement:.1f}%")
    
    print(f"3. èƒä½“å¼‚è´¨æ€§æ˜¾è‘—æå‡äº†ç½‘ç»œåœ¨å¤æ‚æ—¶åºä»»åŠ¡ä¸Šçš„è¡¨ç°")
    print(f"4. ç¨³æ€è°ƒèŠ‚å’Œå¯å¡‘æ€§æœºåˆ¶è¿›ä¸€æ­¥å¢å¼ºäº†å­¦ä¹ èƒ½åŠ›")


def main():
    """ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„åˆ›æ–°æ€§èƒä½“å¼‚è´¨æ€§å®éªŒå¥—ä»¶"""
    print("ğŸ¯ DH-SNN Ultimate: åˆ›æ–°æ€§èƒä½“å¼‚è´¨æ€§å®éªŒå¥—ä»¶")
    print("=" * 60)
    
    try:
        # è¿è¡Œå®éªŒ
        results = run_innovative_experiments()
        
        # æ‰“å°æ€»ç»“æŠ¥å‘Š
        print_summary_report(results)
        
        # å¯è§†åŒ–ç»“æœ
        visualize_results(results)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        save_detailed_results(results)
        
        print(f"\nğŸ‰ åˆ›æ–°æ€§èƒä½“å¼‚è´¨æ€§å®éªŒå¥—ä»¶è¿è¡Œå®Œæˆ!")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°å½“å‰ç›®å½•")
        
    except Exception as e:
        print(f"âŒ å®éªŒè¿è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()