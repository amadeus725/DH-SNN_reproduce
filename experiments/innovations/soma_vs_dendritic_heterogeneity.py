#!/usr/bin/env python3
"""
èƒä½“å¼‚è´¨æ€§vsæ ‘çªå¼‚è´¨æ€§å»¶è¿Ÿå¼‚æˆ–å¯¹æ¯”å®éªŒ
=============================================

åŸºäºSpikingJellyæ¡†æ¶çš„SH-SNN vs DH-SNN vs æ™®é€šSNNä¸‰æ–¹å¯¹æ¯”å®éªŒ
ä½¿ç”¨å»¶è¿Ÿå¼‚æˆ–ä»»åŠ¡éªŒè¯ä¸åŒå¼‚è´¨æ€§æœºåˆ¶çš„æ—¶é—´å¤„ç†èƒ½åŠ›

SH-SNN: èƒä½“å¼‚è´¨æ€§è„‰å†²ç¥ç»ç½‘ç»œ (Soma Heterogeneity SNN)
DH-SNN: æ ‘çªå¼‚è´¨æ€§è„‰å†²ç¥ç»ç½‘ç»œ (Dendritic Heterogeneity SNN)

ä½œè€…: DH-SNN Reproduction Team
æ—¥æœŸ: 2025-06-14
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import time
import json
import math
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent.parent.parent
sys.path.append(str(current_dir))

# SpikingJellyå¯¼å…¥
from spikingjelly.activation_based import neuron, functional, layer, surrogate

# ä¿®å¤å¯¼å…¥é—®é¢˜ï¼šç›´æ¥å®šä¹‰éœ€è¦çš„å‡½æ•°è€Œä¸æ˜¯å¯¼å…¥
def setup_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å®éªŒå¯é‡ç°æ€§"""
    import random
    import numpy as np
    import torch
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# å¤šé«˜æ–¯æ›¿ä»£å‡½æ•°
class MultiGaussianSurrogate(torch.autograd.Function):
    """å¤šé«˜æ–¯æ›¿ä»£å‡½æ•°"""
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.gt(0).float()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        lens = 0.5
        scale = 6.0
        height = 0.15
        gamma = 0.5

        def gaussian(x, mu=0., sigma=0.5):
            return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma

        temp = gaussian(input, mu=0., sigma=lens) * (1. + height) \
             - gaussian(input, mu=lens, sigma=scale * lens) * height \
             - gaussian(input, mu=-lens, sigma=scale * lens) * height

        return grad_input * temp.float() * gamma

multi_gaussian_surrogate = MultiGaussianSurrogate.apply

# æ•°æ®ç”Ÿæˆå‡½æ•°
def generate_delayed_xor_data(batch_size, seq_length, delay, num_samples=1000):
    """
    ç”Ÿæˆå»¶è¿Ÿå¼‚æˆ–ä»»åŠ¡æ•°æ® - å®Œå…¨å¤åˆ¶åŸè®ºæ–‡çš„å•è„‰å†²æ–¹å¼
    
    å‚æ•°:
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_length: åºåˆ—é•¿åº¦
        delay: å»¶è¿Ÿæ­¥æ•°
        num_samples: æ ·æœ¬æ•°é‡
        
    è¿”å›:
        data: è¾“å…¥è„‰å†²åºåˆ— [æ ·æœ¬æ•°, æ—¶é—´æ­¥, è¾“å…¥ç»´åº¦]
        labels: ç›®æ ‡æ ‡ç­¾ [æ ·æœ¬æ•°]
    """
    data = torch.zeros(num_samples, seq_length, 2)
    labels = torch.zeros(num_samples, dtype=torch.long)
    
    for i in range(num_samples):
        # åœ¨åºåˆ—å¼€å§‹å¤„ç”Ÿæˆä¸¤ä¸ªéšæœºè„‰å†²
        signal1_time = np.random.randint(5, 15)  # ç¬¬ä¸€ä¸ªä¿¡å·çš„æ—¶é—´
        signal2_time = signal1_time + delay      # ç¬¬äºŒä¸ªä¿¡å·å»¶è¿Ÿdelayæ­¥
        
        # ç¡®ä¿ç¬¬äºŒä¸ªä¿¡å·åœ¨åºåˆ—èŒƒå›´å†…
        if signal2_time < seq_length - 10:            # ç”Ÿæˆè„‰å†²
            signal1_value = np.random.choice([0, 1])
            signal2_value = np.random.choice([0, 1])
            
            data[i, signal1_time, 0] = float(signal1_value)
            data[i, signal2_time, 1] = float(signal2_value)
            
            # å¼‚æˆ–æ ‡ç­¾
            labels[i] = int(signal1_value ^ signal2_value)
        else:
            # å¦‚æœå»¶è¿Ÿå¤ªé•¿ï¼Œæ ‡ç­¾è®¾ä¸º0
            signal1_value = np.random.choice([0, 1])
            data[i, signal1_time, 0] = float(signal1_value)
            labels[i] = 0
    
    return data, labels

def create_delayed_xor_datasets(delays):
    """åˆ›å»ºä¸åŒå»¶è¿Ÿçš„å¼‚æˆ–æ•°æ®é›† - æŒ‰ç…§åŸè®ºæ–‡é…ç½®"""
    datasets = {}
    
    for delay in delays:
        print(f"ğŸ“Š ç”Ÿæˆå»¶è¿Ÿ{delay}æ­¥çš„å¼‚æˆ–æ•°æ®...")
          # ä¼˜åŒ–çš„æ ·æœ¬æ•°é‡ï¼Œå¹³è¡¡è®­ç»ƒæ•ˆæœå’Œé€Ÿåº¦
        train_data, train_labels = generate_delayed_xor_data(BATCH_SIZE, SEQ_LENGTH, delay, num_samples=1000)  # å‡å°‘è®­ç»ƒæ ·æœ¬
        test_data, test_labels = generate_delayed_xor_data(BATCH_SIZE, SEQ_LENGTH, delay, num_samples=200)   # å‡å°‘æµ‹è¯•æ ·æœ¬
        
        train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)
        test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)
        
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
        
        datasets[delay] = {
            'train_loader': train_loader,
            'test_loader': test_loader,
            'train_size': len(train_data),
            'test_size': len(test_data)
        }
        
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_data)}, æµ‹è¯•æ ·æœ¬: {len(test_data)}")
    
    return datasets

# æ™®é€šSNNæ¨¡å‹
class DelayedXOR_Vanilla_SNN(nn.Module):
    """æ™®é€šSNNæ¨¡å‹"""
    def __init__(self, input_size=2, hidden_size=32, output_size=1):
        super(DelayedXOR_Vanilla_SNN, self).__init__()
        
        self.fc1 = layer.Linear(input_size, hidden_size)
        self.lif1 = neuron.LIFNode(tau=2.0, v_threshold=1.0, surrogate_function=surrogate.ATan(), step_mode='s')
        
        self.fc2 = layer.Linear(hidden_size, output_size)
        self.lif2 = neuron.LIFNode(tau=2.0, v_threshold=1.0, surrogate_function=surrogate.ATan(), step_mode='s')
    
    def forward(self, x):
        batch_size, seq_len, input_dim = x.shape
        functional.reset_net(self)
        
        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :]
            h1 = self.fc1(x_t)
            s1 = self.lif1(h1)
            h2 = self.fc2(s1)
            s2 = self.lif2(h2)
            outputs.append(s2)
        
        # å»¶è¿Ÿå¼‚æˆ–ä»»åŠ¡çš„å†³ç­–æ—¶é—´ç‚¹ - ç»Ÿä¸€ä½¿ç”¨ååŠæ®µ
        decision_start = seq_len//2
        integrated_output = torch.stack(outputs[decision_start:], dim=1).sum(dim=1)
        return integrated_output

print("ğŸš€ èƒä½“å¼‚è´¨æ€§ vs æ ‘çªå¼‚è´¨æ€§å»¶è¿Ÿå¼‚æˆ–å¯¹æ¯”å®éªŒ")
print("="*60)

# å®éªŒå‚æ•° - æ›´ä¿å®ˆçš„ä¼˜åŒ–é…ç½®
BATCH_SIZE = 32      # åŸè®ºæ–‡æ‰¹æ¬¡å¤§å°ï¼Œæ›´ç¨³å®š
LEARNING_RATE = 1e-3 # æ¢å¤åŸè®ºæ–‡å­¦ä¹ ç‡
NUM_EPOCHS = 50      # å‡å°‘è®­ç»ƒè½®æ•°
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å»¶è¿Ÿå¼‚æˆ–ä»»åŠ¡å‚æ•° - å¹³è¡¡ç‰ˆæœ¬  
SEQ_LENGTH = 300     # é€‚ä¸­çš„åºåˆ—é•¿åº¦ï¼Œç¡®ä¿é•¿æœŸè®°å¿†æµ‹è¯•
INPUT_SIZE = 2       
HIDDEN_SIZE = 32     
OUTPUT_SIZE = 1      
DELAY_RANGE = [25, 100]  # åªæµ‹è¯•ä¸¤ä¸ªå…³é”®å»¶è¿Ÿï¼šçŸ­æœŸå’Œé•¿æœŸ

# ==================== èƒä½“å¼‚è´¨æ€§æ¨¡å‹ ====================

class DelayedXOR_SH_SNN(nn.Module):
    """
    èƒä½“å¼‚è´¨æ€§SNNæ¨¡å‹ (SH-SNN)
    åœ¨èƒä½“å±‚é¢å¼•å…¥å¼‚è´¨æ€§ï¼Œä¸åŒç¥ç»å…ƒå…·æœ‰ä¸åŒçš„æ—¶é—´å¸¸æ•°
    """
    
    def __init__(self, input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE):
        super(DelayedXOR_SH_SNN, self).__init__()
        
        print(f"ğŸ§  åˆ›å»ºèƒä½“å¼‚è´¨æ€§SH-SNNæ¨¡å‹:")
        print(f"   è¾“å…¥ç»´åº¦: {input_size}")
        print(f"   éšè—ç»´åº¦: {hidden_size}")
        print(f"   è¾“å‡ºç»´åº¦: {output_size}")
        print(f"   å¼‚è´¨æ€§ç±»å‹: èƒä½“å¼‚è´¨æ€§ (ä¸åŒç¥ç»å…ƒä¸åŒæ—¶é—´å¸¸æ•°)")
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # è¾“å…¥åˆ°éšè—å±‚
        self.fc1 = layer.Linear(input_size, hidden_size)
        
        # èƒä½“å¼‚è´¨æ€§ï¼šæ¯ä¸ªç¥ç»å…ƒæœ‰ç‹¬ç«‹çš„æ—¶é—´å¸¸æ•°
        # ä¸€åŠç¥ç»å…ƒç”¨äºçŸ­æœŸè®°å¿†ï¼Œä¸€åŠç”¨äºé•¿æœŸè®°å¿†
        self.tau_m = nn.Parameter(torch.empty(hidden_size))
        # åˆå§‹åŒ–ï¼šå‰åŠéƒ¨åˆ†ç¥ç»å…ƒå¿«é€Ÿæ—¶é—´å¸¸æ•°ï¼ŒååŠéƒ¨åˆ†æ…¢é€Ÿæ—¶é—´å¸¸æ•°
        nn.init.uniform_(self.tau_m[:hidden_size//2], 0.0, 2.0)  # å¿«é€Ÿç¥ç»å…ƒ
        nn.init.uniform_(self.tau_m[hidden_size//2:], 3.0, 6.0)  # æ…¢é€Ÿç¥ç»å…ƒ
        
        # è¾“å‡ºå±‚
        self.fc2 = layer.Linear(hidden_size, output_size)
        
        # ç¥ç»å…ƒçŠ¶æ€
        self.membrane_potential = None
        self.spike_output = None
        
        print("âœ… èƒä½“å¼‚è´¨æ€§SH-SNNæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    def reset_states(self, batch_size):
        """é‡ç½®ç¥ç»å…ƒçŠ¶æ€"""
        self.membrane_potential = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
        self.spike_output = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
    
    def surrogate_gradient(self, x):
        """ä½¿ç”¨å¤šé«˜æ–¯æ›¿ä»£å‡½æ•°"""
        return multi_gaussian_surrogate(x)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len, input_dim = x.shape
        
        # é‡ç½®çŠ¶æ€
        self.reset_states(batch_size)
        
        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :]  # [batch, input_size]
            
            # è¾“å…¥åˆ°éšè—å±‚
            input_current = self.fc1(x_t)
            
            # èƒä½“å¼‚è´¨æ€§ï¼šæ¯ä¸ªç¥ç»å…ƒç‹¬ç«‹çš„æ—¶é—´å¸¸æ•°
            alpha = torch.sigmoid(self.tau_m)  # [hidden_size]
            v_th = 1.0
            
            # è†œç”µä½æ›´æ–°ï¼ˆæ¯ä¸ªç¥ç»å…ƒç‹¬ç«‹çš„è¡°å‡ï¼‰
            self.membrane_potential = (
                alpha * self.membrane_potential + 
                (1 - alpha) * input_current - 
                v_th * self.spike_output
            )
            
            # è„‰å†²ç”Ÿæˆ
            spike_input = self.membrane_potential - v_th
            self.spike_output = self.surrogate_gradient(spike_input)
            
            outputs.append(self.spike_output)
          # å»¶è¿Ÿå¼‚æˆ–ä»»åŠ¡çš„å†³ç­–æ—¶é—´ç‚¹ - ç»Ÿä¸€ä½¿ç”¨ååŠæ®µ
        decision_start = seq_len//2
        integrated_output = torch.stack(outputs[decision_start:], dim=1).sum(dim=1)
        
        # è¾“å‡ºå±‚
        final_output = self.fc2(integrated_output)
        
        return final_output

# ==================== æ”¹è¿›çš„èƒä½“å¼‚è´¨æ€§æ¨¡å‹ ====================

class DelayedXOR_SH_SNN_Improved(nn.Module):
    """
    æ”¹è¿›çš„èƒä½“å¼‚è´¨æ€§SNNæ¨¡å‹ (SH-SNN)
    
    è®¾è®¡ç†å¿µï¼š
    1. å¤šä¸ªä¸“é—¨åŒ–çš„ç¥ç»å…ƒç¾¤ä½“ï¼Œæ¯ä¸ªç¾¤ä½“å¤„ç†ä¸åŒæ—¶é—´å°ºåº¦
    2. èƒä½“å±‚é¢çš„ä¿¡æ¯æ•´åˆï¼Œç±»ä¼¼DH-SNNçš„åˆ†æ”¯æ•´åˆ
    3. ä¸DH-SNNåœ¨å¤æ‚åº¦å’Œå‚æ•°é‡ä¸Šä¿æŒå¯æ¯”æ€§
    """
    
    def __init__(self, input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE, num_soma_groups=2):
        super(DelayedXOR_SH_SNN_Improved, self).__init__()
        
        print(f"ğŸ§  åˆ›å»ºæ”¹è¿›çš„èƒä½“å¼‚è´¨æ€§SH-SNNæ¨¡å‹:")
        print(f"   è¾“å…¥ç»´åº¦: {input_size}")
        print(f"   éšè—ç»´åº¦: {hidden_size}")
        print(f"   è¾“å‡ºç»´åº¦: {output_size}")
        print(f"   èƒä½“ç¾¤ä½“æ•°: {num_soma_groups}")
        print(f"   å¼‚è´¨æ€§ç±»å‹: èƒä½“ç¾¤ä½“å¼‚è´¨æ€§ (ä¸åŒç¾¤ä½“ä¸åŒæ—¶é—´ç‰¹æ€§)")
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_soma_groups = num_soma_groups
        self.group_size = hidden_size // num_soma_groups
        
        # ä¸ºæ¯ä¸ªèƒä½“ç¾¤ä½“åˆ›å»ºç‹¬ç«‹çš„è¾“å…¥å¤„ç†
        self.soma_group_layers = nn.ModuleList()
        for i in range(num_soma_groups):
            self.soma_group_layers.append(
                layer.Linear(input_size, self.group_size, bias=False)
            )
          # æ¯ä¸ªèƒä½“ç¾¤ä½“æœ‰ç‹¬ç«‹çš„æ—¶é—´å¸¸æ•°
        self.tau_m_groups = nn.Parameter(torch.empty(num_soma_groups, self.group_size))
        # ç¾¤ä½“1ç”¨äºé•¿æœŸè®°å¿†ï¼Œç¾¤ä½“2ç”¨äºçŸ­æœŸè®°å¿† - ä½¿ç”¨æ›´åˆç†çš„èŒƒå›´
        nn.init.uniform_(self.tau_m_groups[0], 1.0, 3.0)  # ç¾¤ä½“1ï¼šä¸­ç­‰é€Ÿåº¦ï¼ˆé•¿æœŸè®°å¿†ï¼‰
        if num_soma_groups > 1:
            nn.init.uniform_(self.tau_m_groups[1], -1.0, 1.0)  # ç¾¤ä½“2ï¼šå¿«é€Ÿï¼ˆçŸ­æœŸè®°å¿†ï¼‰
        
        # èƒä½“æ•´åˆå±‚ - ç±»ä¼¼DH-SNNçš„èƒä½“æ•´åˆåŠŸèƒ½
        self.soma_integration_tau = nn.Parameter(torch.empty(hidden_size).uniform_(0.0, 2.0))
        
        # è¾“å‡ºå±‚
        self.output_layer = layer.Linear(hidden_size, output_size)
        
        # ç¥ç»å…ƒçŠ¶æ€
        self.group_potentials = None  # æ¯ä¸ªç¾¤ä½“çš„è†œç”µä½
        self.group_spikes = None      # æ¯ä¸ªç¾¤ä½“çš„è„‰å†²
        self.integrated_potential = None  # èƒä½“æ•´åˆåçš„è†œç”µä½
        self.final_spike = None       # æœ€ç»ˆè¾“å‡ºè„‰å†²
        
        print("âœ… æ”¹è¿›çš„èƒä½“å¼‚è´¨æ€§SH-SNNæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    def reset_states(self, batch_size):
        """é‡ç½®ç¥ç»å…ƒçŠ¶æ€"""
        self.group_potentials = [
            torch.zeros(batch_size, self.group_size).to(DEVICE)
            for _ in range(self.num_soma_groups)
        ]
        self.group_spikes = [
            torch.zeros(batch_size, self.group_size).to(DEVICE)
            for _ in range(self.num_soma_groups)
        ]
        self.integrated_potential = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
        self.final_spike = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
    
    def surrogate_gradient(self, x):
        """ä½¿ç”¨å¤šé«˜æ–¯æ›¿ä»£å‡½æ•°"""
        return multi_gaussian_surrogate(x)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len, input_dim = x.shape
        
        # é‡ç½®çŠ¶æ€
        self.reset_states(batch_size)
        
        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :]  # [batch, input_size]
            
            # å¤„ç†å„ä¸ªèƒä½“ç¾¤ä½“
            group_outputs = []
            for i in range(self.num_soma_groups):
                # ç¾¤ä½“è¾“å…¥å¤„ç†
                group_input = self.soma_group_layers[i](x_t)
                
                # ç¾¤ä½“ç‰¹åŒ–çš„è†œç”µä½æ›´æ–°
                alpha_group = torch.sigmoid(self.tau_m_groups[i])
                v_th = 1.0
                
                self.group_potentials[i] = (
                    alpha_group * self.group_potentials[i] + 
                    (1 - alpha_group) * group_input - 
                    v_th * self.group_spikes[i]
                )
                
                # ç¾¤ä½“è„‰å†²ç”Ÿæˆ
                spike_input = self.group_potentials[i] - v_th
                self.group_spikes[i] = self.surrogate_gradient(spike_input)
                
                group_outputs.append(self.group_spikes[i])
            
            # èƒä½“æ•´åˆ - ç±»ä¼¼DH-SNNçš„åˆ†æ”¯æ•´åˆ
            integrated_input = torch.cat(group_outputs, dim=1)  # [batch, hidden_size]
            
            # èƒä½“å±‚é¢çš„æ•´åˆåŠ¨æ€
            alpha_soma = torch.sigmoid(self.soma_integration_tau)
            v_th = 1.0
            
            self.integrated_potential = (
                alpha_soma * self.integrated_potential + 
                (1 - alpha_soma) * integrated_input - 
                v_th * self.final_spike
            )
            
            # æœ€ç»ˆè„‰å†²ç”Ÿæˆ
            final_spike_input = self.integrated_potential - v_th
            self.final_spike = self.surrogate_gradient(final_spike_input)
            
            outputs.append(self.final_spike)
        
        # å»¶è¿Ÿå¼‚æˆ–ä»»åŠ¡éœ€è¦åœ¨åºåˆ—æœ«å°¾åšå†³ç­–ï¼Œä½¿ç”¨æœ€å1/4çš„æ—¶é—´æ­¥
        decision_start = max(seq_len - seq_len//4, seq_len//2)
        integrated_output = torch.stack(outputs[decision_start:], dim=1).sum(dim=1)
        
        # è¾“å‡ºå±‚
        final_output = self.output_layer(integrated_output)
        
        return final_output

# åŸå§‹ç®€å•ç‰ˆæœ¬æ”¹åä¸ºLegacy
class DelayedXOR_SH_SNN_Legacy(nn.Module):
    """
    åŸå§‹èƒä½“å¼‚è´¨æ€§SNNæ¨¡å‹ (ä¿ç•™ç”¨äºå¯¹æ¯”)
    """
    
    def __init__(self, input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE):
        super(DelayedXOR_SH_SNN_Legacy, self).__init__()
        
        print(f"ğŸ§  åˆ›å»ºåŸå§‹èƒä½“å¼‚è´¨æ€§SH-SNNæ¨¡å‹ (é—ç•™):")
        print(f"   è¾“å…¥ç»´åº¦: {input_size}")
        print(f"   éšè—ç»´åº¦: {hidden_size}")
        print(f"   è¾“å‡ºç»´åº¦: {output_size}")
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
          # æ™®é€šSNNï¼šæŒ‰ç…§åŸè®ºæ–‡ä½¿ç”¨å›ºå®šæ—¶é—´å¸¸æ•°
        self.fc1 = layer.Linear(input_size, hidden_size)
        self.lif1 = neuron.LIFNode(tau=2.0, v_threshold=1.0, surrogate_function=surrogate.ATan(), step_mode='s')
        
        self.fc2 = layer.Linear(hidden_size, output_size)
        self.lif2 = neuron.LIFNode(tau=2.0, v_threshold=1.0, surrogate_function=surrogate.ATan(), step_mode='s')
    
    def forward(self, x):
        batch_size, seq_len, input_dim = x.shape
        functional.reset_net(self)
        
        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :]
            h1 = self.fc1(x_t)
            s1 = self.lif1(h1)
            h2 = self.fc2(s1)
            s2 = self.lif2(h2)
            outputs.append(s2)
          # å»¶è¿Ÿå¼‚æˆ–ä»»åŠ¡éœ€è¦åœ¨åºåˆ—æœ«å°¾åšå†³ç­–ï¼Œä½¿ç”¨æœ€å1/4çš„æ—¶é—´æ­¥
        decision_start = max(seq_len - seq_len//4, seq_len//2)
        integrated_output = torch.stack(outputs[decision_start:], dim=1).sum(dim=1)
        return integrated_output

# ==================== æ ‘çªå¼‚è´¨æ€§æ¨¡å‹ ====================

class DelayedXOR_DH_SNN(nn.Module):
    """
    æ ‘çªå¼‚è´¨æ€§SNNæ¨¡å‹ (DH-SNN)
    åœ¨æ ‘çªå±‚é¢å¼•å…¥å¼‚è´¨æ€§ï¼Œä¸åŒåˆ†æ”¯å…·æœ‰ä¸åŒçš„æ—¶é—´å¸¸æ•°
    """
    
    def __init__(self, input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE, num_branches=2):
        super(DelayedXOR_DH_SNN, self).__init__()
        
        print(f"ğŸŒ³ åˆ›å»ºæ ‘çªå¼‚è´¨æ€§DH-SNNæ¨¡å‹:")
        print(f"   è¾“å…¥ç»´åº¦: {input_size}")
        print(f"   éšè—ç»´åº¦: {hidden_size}")
        print(f"   è¾“å‡ºç»´åº¦: {output_size}")
        print(f"   åˆ†æ”¯æ•°é‡: {num_branches}")
        print(f"   å¼‚è´¨æ€§ç±»å‹: æ ‘çªå¼‚è´¨æ€§ (ä¸åŒåˆ†æ”¯ä¸åŒæ—¶é—´å¸¸æ•°)")
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_branches = num_branches
        
        # åˆ†æ”¯çº¿æ€§å±‚
        self.branch_layers = nn.ModuleList()
        for i in range(num_branches):
            self.branch_layers.append(
                layer.Linear(input_size, hidden_size // num_branches, bias=False)
            )        # æ ‘çªå¼‚è´¨æ€§ï¼šä¸¥æ ¼æŒ‰ç…§åŸè®ºæ–‡åˆå§‹åŒ–
        # tau_n: æ ‘çªæ—¶é—´å¸¸æ•°ï¼ŒLargeé…ç½®(2,6)
        self.tau_n = nn.Parameter(torch.empty(num_branches, hidden_size // num_branches).uniform_(2, 6))
        # tau_m: è†œç”µä½æ—¶é—´å¸¸æ•°ï¼ŒMediumé…ç½®(0,4)
        self.tau_m = nn.Parameter(torch.empty(hidden_size).uniform_(0, 4))
        
        # è¾“å‡ºå±‚
        self.output_layer = layer.Linear(hidden_size, output_size)
        
        # ç¥ç»å…ƒçŠ¶æ€
        self.dendritic_currents = None
        self.membrane_potential = None
        self.spike_output = None
        
        print("âœ… æ ‘çªå¼‚è´¨æ€§DH-SNNæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    def reset_states(self, batch_size):
        """é‡ç½®ç¥ç»å…ƒçŠ¶æ€"""
        self.dendritic_currents = [
            torch.zeros(batch_size, self.hidden_size // self.num_branches).to(DEVICE)
            for _ in range(self.num_branches)
        ]
        self.membrane_potential = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
        self.spike_output = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
    
    def surrogate_gradient(self, x):
        """ä½¿ç”¨å¤šé«˜æ–¯æ›¿ä»£å‡½æ•°"""
        return multi_gaussian_surrogate(x)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len, input_dim = x.shape
        
        # é‡ç½®çŠ¶æ€
        self.reset_states(batch_size)
        
        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :]  # [batch, input_size]
            
            # å¤„ç†å„ä¸ªåˆ†æ”¯
            branch_outputs = []
            for i in range(self.num_branches):
                # åˆ†æ”¯çº¿æ€§å˜æ¢
                branch_input = self.branch_layers[i](x_t)
                
                # æ ‘çªæ—¶é—´å¸¸æ•°æ›´æ–°
                beta = torch.sigmoid(self.tau_n[i])
                self.dendritic_currents[i] = (
                    beta * self.dendritic_currents[i] + 
                    (1 - beta) * branch_input
                )
                
                branch_outputs.append(self.dendritic_currents[i])
            
            # åˆå¹¶åˆ†æ”¯è¾“å‡º
            total_current = torch.cat(branch_outputs, dim=1)  # [batch, hidden_size]
            
            # è†œç”µä½æ›´æ–°
            alpha = torch.sigmoid(self.tau_m)
            v_th = 1.0
            
            self.membrane_potential = (
                alpha * self.membrane_potential + 
                (1 - alpha) * total_current - 
                v_th * self.spike_output
            )
              # è„‰å†²ç”Ÿæˆ
            spike_input = self.membrane_potential - v_th
            self.spike_output = self.surrogate_gradient(spike_input)
            
            outputs.append(self.spike_output)
        
        # å»¶è¿Ÿå¼‚æˆ–ä»»åŠ¡éœ€è¦åœ¨åºåˆ—æœ«å°¾åšå†³ç­–ï¼Œä½¿ç”¨æœ€å1/4çš„æ—¶é—´æ­¥
        decision_start = max(seq_len - seq_len//4, seq_len//2)
        integrated_output = torch.stack(outputs[decision_start:], dim=1).sum(dim=1)
        
        # è¾“å‡ºå±‚
        final_output = self.output_layer(integrated_output)
        
        return final_output

# ==================== è®­ç»ƒå’Œæµ‹è¯•å‡½æ•° ====================

def train_delayed_xor_model(model, train_loader, test_loader, model_name, num_epochs=NUM_EPOCHS):
    """è®­ç»ƒå»¶è¿Ÿå¼‚æˆ–æ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {model_name}")
    print("-" * 50)
    
    model = model.to(DEVICE)
    
    # ä¼˜åŒ–å™¨é…ç½®
    if isinstance(model, (DelayedXOR_DH_SNN, DelayedXOR_SH_SNN, DelayedXOR_SH_SNN_Improved)):
        # å¼‚è´¨æ€§SNNä½¿ç”¨åˆ†å±‚å­¦ä¹ ç‡
        base_params = []
        tau_params = []
        
        for name, param in model.named_parameters():
            if 'tau_' in name:
                tau_params.append(param)
            else:
                base_params.append(param)
        
        optimizer = torch.optim.Adam([
            {'params': base_params, 'lr': LEARNING_RATE},
            {'params': tau_params, 'lr': LEARNING_RATE * 2},  # æ—¶é—´å¸¸æ•°ç”¨2å€å­¦ä¹ ç‡
        ])
    else:
        # æ™®é€šSNNä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
    criterion = nn.BCEWithLogitsLoss()
    
    best_test_acc = 0.0
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_data, batch_labels in train_loader:
            batch_data, batch_labels = batch_data.to(DEVICE), batch_labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(batch_data)
            
            # è°ƒæ•´è¾“å‡ºå’Œæ ‡ç­¾å½¢çŠ¶
            outputs = outputs.squeeze(-1)
            batch_labels = batch_labels.float()
            
            loss = criterion(outputs, batch_labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            running_loss += loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            predicted = (torch.sigmoid(outputs) > 0.5).float()
            total += batch_labels.size(0)
            correct += (predicted == batch_labels).sum().item()
        
        scheduler.step()
        
        train_acc = 100 * correct / total
        train_loss = running_loss / len(train_loader)
        
        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for batch_data, batch_labels in test_loader:
                batch_data, batch_labels = batch_data.to(DEVICE), batch_labels.to(DEVICE)
                
                outputs = model(batch_data)
                outputs = outputs.squeeze(-1)
                batch_labels = batch_labels.float()
                
                predicted = (torch.sigmoid(outputs) > 0.5).float()
                test_total += batch_labels.size(0)
                test_correct += (predicted == batch_labels).sum().item()
        
        test_acc = 100 * test_correct / test_total
        
        if test_acc > best_test_acc:
            best_test_acc = test_acc
          # æ‰“å°è¿›åº¦ - æ›´é¢‘ç¹çš„åé¦ˆ
        if epoch % 10 == 0 or epoch == num_epochs - 1:
            print(f'è½®æ¬¡ [{epoch+1}/{num_epochs}]: è®­ç»ƒå‡†ç¡®ç‡={train_acc:.1f}%, æµ‹è¯•å‡†ç¡®ç‡={test_acc:.1f}%, æœ€ä½³={best_test_acc:.1f}%')
    
    return {
        'best_test_acc': best_test_acc,
        'final_test_acc': test_acc
    }

# ==================== ä¸»å®éªŒå‡½æ•° ====================

def run_heterogeneity_comparison_experiment():
    """è¿è¡Œèƒä½“å¼‚è´¨æ€§vsæ ‘çªå¼‚è´¨æ€§å¯¹æ¯”å®éªŒ"""
    
    print("=" * 80)
    print("ğŸ§ ğŸŒ³ èƒä½“å¼‚è´¨æ€§ vs æ ‘çªå¼‚è´¨æ€§å»¶è¿Ÿå¼‚æˆ–å¯¹æ¯”å®éªŒ")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    setup_seed(42)
    
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        print("ğŸ“Š åˆ›å»ºå»¶è¿Ÿå¼‚æˆ–æ•°æ®é›†...")
        datasets = create_delayed_xor_datasets(DELAY_RANGE)
        
        all_results = {}
        
        # å¯¹æ¯ä¸ªå»¶è¿Ÿè®¾ç½®è¿›è¡Œä¸‰æ–¹å¯¹æ¯”å®éªŒ
        for delay in DELAY_RANGE:
            print(f"\nğŸ”¬ å®éªŒå»¶è¿Ÿ={delay}æ­¥çš„å¼‚æˆ–ä»»åŠ¡")
            print("=" * 50)
            
            train_loader = datasets[delay]['train_loader']
            test_loader = datasets[delay]['test_loader']
            
            # åˆ›å»ºä¸‰ç§æ¨¡å‹ - ä½¿ç”¨æ”¹è¿›çš„SH-SNN
            sh_snn_model = DelayedXOR_SH_SNN_Improved()  # æ”¹è¿›çš„èƒä½“å¼‚è´¨æ€§
            dh_snn_model = DelayedXOR_DH_SNN()  # æ ‘çªå¼‚è´¨æ€§
            vanilla_snn_model = DelayedXOR_Vanilla_SNN()  # æ™®é€šSNN
            
            print(f"\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
            print(f"   SH-SNNå‚æ•°: {sum(p.numel() for p in sh_snn_model.parameters()):,}")
            print(f"   DH-SNNå‚æ•°: {sum(p.numel() for p in dh_snn_model.parameters()):,}")
            print(f"   æ™®é€šSNNå‚æ•°: {sum(p.numel() for p in vanilla_snn_model.parameters()):,}")
            
            # è®­ç»ƒä¸‰ç§æ¨¡å‹
            print(f"\nğŸš€ è®­ç»ƒå»¶è¿Ÿ{delay}æ­¥çš„ä¸‰ç§æ¨¡å‹...")
            
            # è®­ç»ƒæ”¹è¿›çš„SH-SNN
            sh_results = train_delayed_xor_model(
                sh_snn_model, train_loader, test_loader, f"SH-SNN-Improved (å»¶è¿Ÿ{delay})", NUM_EPOCHS
            )
            
            # è®­ç»ƒDH-SNN
            dh_results = train_delayed_xor_model(
                dh_snn_model, train_loader, test_loader, f"DH-SNN (å»¶è¿Ÿ{delay})", NUM_EPOCHS
            )
            
            # è®­ç»ƒæ™®é€šSNN
            vanilla_results = train_delayed_xor_model(
                vanilla_snn_model, train_loader, test_loader, f"æ™®é€šSNN (å»¶è¿Ÿ{delay})", NUM_EPOCHS
            )
            
            # ä¿å­˜ç»“æœ
            all_results[delay] = {
                'sh_snn': sh_results,
                'dh_snn': dh_results,
                'vanilla_snn': vanilla_results,
                'sh_vs_vanilla': sh_results['best_test_acc'] - vanilla_results['best_test_acc'],
                'dh_vs_vanilla': dh_results['best_test_acc'] - vanilla_results['best_test_acc'],
                'dh_vs_sh': dh_results['best_test_acc'] - sh_results['best_test_acc']
            }
            
            print(f"\nğŸ“ˆ å»¶è¿Ÿ{delay}æ­¥ç»“æœ:")
            print(f"   SH-SNNæœ€ä½³å‡†ç¡®ç‡: {sh_results['best_test_acc']:.1f}%")
            print(f"   DH-SNNæœ€ä½³å‡†ç¡®ç‡: {dh_results['best_test_acc']:.1f}%")
            print(f"   æ™®é€šSNNæœ€ä½³å‡†ç¡®ç‡: {vanilla_results['best_test_acc']:.1f}%")
            print(f"   SH-SNN vs æ™®é€šSNN: {all_results[delay]['sh_vs_vanilla']:+.1f}%")
            print(f"   DH-SNN vs æ™®é€šSNN: {all_results[delay]['dh_vs_vanilla']:+.1f}%")
            print(f"   DH-SNN vs SH-SNN: {all_results[delay]['dh_vs_sh']:+.1f}%")
        
        # æ€»ç»“æ‰€æœ‰ç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ¯ èƒä½“å¼‚è´¨æ€§ vs æ ‘çªå¼‚è´¨æ€§å¯¹æ¯”æ€»ç»“")
        print("=" * 80)
        
        print("å»¶è¿Ÿæ­¥æ•° | SH-SNN | DH-SNN | æ™®é€šSNN | SHæå‡ | DHæå‡ | DH>SH")
        print("-" * 70)
        
        total_sh_acc = 0
        total_dh_acc = 0
        total_vanilla_acc = 0
        
        for delay in DELAY_RANGE:
            sh_acc = all_results[delay]['sh_snn']['best_test_acc']
            dh_acc = all_results[delay]['dh_snn']['best_test_acc']
            vanilla_acc = all_results[delay]['vanilla_snn']['best_test_acc']
            sh_improvement = all_results[delay]['sh_vs_vanilla']
            dh_improvement = all_results[delay]['dh_vs_vanilla']
            dh_vs_sh = all_results[delay]['dh_vs_sh']
            
            print(f"{delay:8d} | {sh_acc:6.1f}% | {dh_acc:6.1f}% | {vanilla_acc:7.1f}% | {sh_improvement:+5.1f}% | {dh_improvement:+5.1f}% | {dh_vs_sh:+5.1f}%")
            
            total_sh_acc += sh_acc
            total_dh_acc += dh_acc
            total_vanilla_acc += vanilla_acc
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_sh_acc = total_sh_acc / len(DELAY_RANGE)
        avg_dh_acc = total_dh_acc / len(DELAY_RANGE)
        avg_vanilla_acc = total_vanilla_acc / len(DELAY_RANGE)
        avg_sh_improvement = avg_sh_acc - avg_vanilla_acc
        avg_dh_improvement = avg_dh_acc - avg_vanilla_acc
        avg_dh_vs_sh = avg_dh_acc - avg_sh_acc
        
        print("-" * 70)
        print(f"å¹³å‡     | {avg_sh_acc:6.1f}% | {avg_dh_acc:6.1f}% | {avg_vanilla_acc:7.1f}% | {avg_sh_improvement:+5.1f}% | {avg_dh_improvement:+5.1f}% | {avg_dh_vs_sh:+5.1f}%")
        
        # ä¿å­˜ç»“æœ
        results_path = Path("results/heterogeneity_comparison_results.json")
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        final_results = {
            'experiment_info': {
                'name': 'èƒä½“å¼‚è´¨æ€§vsæ ‘çªå¼‚è´¨æ€§å¯¹æ¯”å®éªŒ',
                'framework': 'SpikingJelly',
                'date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'delay_range': DELAY_RANGE,
                'seq_length': SEQ_LENGTH,
                'num_epochs': NUM_EPOCHS,
                'device': str(DEVICE)
            },
            'results_by_delay': all_results,
            'summary': {
                'avg_sh_snn_acc': avg_sh_acc,
                'avg_dh_snn_acc': avg_dh_acc,
                'avg_vanilla_snn_acc': avg_vanilla_acc,
                'avg_sh_improvement': avg_sh_improvement,
                'avg_dh_improvement': avg_dh_improvement,
                'avg_dh_vs_sh': avg_dh_vs_sh
            }
        }
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
        # åˆ†æç»“æœ
        print(f"\nğŸ“ˆ å®éªŒç»“è®º:")
        print(f"ğŸ§  èƒä½“å¼‚è´¨æ€§(SH-SNN)å¹³å‡æå‡: {avg_sh_improvement:+.1f}%")
        print(f"ğŸŒ³ æ ‘çªå¼‚è´¨æ€§(DH-SNN)å¹³å‡æå‡: {avg_dh_improvement:+.1f}%")
        print(f"ğŸ¥‡ DH-SNN vs SH-SNNä¼˜åŠ¿: {avg_dh_vs_sh:+.1f}%")
        
        if avg_dh_vs_sh > 2:
            print("ğŸ‰ æ ‘çªå¼‚è´¨æ€§æ˜æ˜¾ä¼˜äºèƒä½“å¼‚è´¨æ€§ï¼")
        elif avg_dh_vs_sh > 0:
            print("âœ… æ ‘çªå¼‚è´¨æ€§ç•¥ä¼˜äºèƒä½“å¼‚è´¨æ€§")
        elif avg_dh_vs_sh > -2:
            print("ğŸ¤ ä¸¤ç§å¼‚è´¨æ€§æœºåˆ¶æ€§èƒ½ç›¸å½“")
        else:
            print("ğŸ’¡ èƒä½“å¼‚è´¨æ€§åœ¨æŸäº›æ¡ä»¶ä¸‹æ›´ä¼˜")
        
        return final_results
        
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = run_heterogeneity_comparison_experiment()
    if results:
        print(f"\nğŸ èƒä½“å¼‚è´¨æ€§vsæ ‘çªå¼‚è´¨æ€§å¯¹æ¯”å®éªŒæˆåŠŸå®Œæˆ!")
    else:
        print(f"\nâŒ å¯¹æ¯”å®éªŒå¤±è´¥")